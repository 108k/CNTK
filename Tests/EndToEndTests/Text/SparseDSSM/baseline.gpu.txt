CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz
    Hardware threads: 12
    Total Memory: 57700428 kB
-------------------------------------------------------------------
=== Running mpiexec -n 4 /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu DeviceId=0 timestamping=true numCPUThreads=3 stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:50:53

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:50:53

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
Changed current directory to /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData
Changed current directory to /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:50:53

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
Changed current directory to /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:50:53

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
Changed current directory to /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData
--------------------------------------------------------------------------
[[5510,1],2]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: 4bb8be993ee1

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (0) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (3) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (1) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (2) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
12/09/2017 11:50:53: Redirecting stderr to file /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr_train.logrank0
12/09/2017 11:50:53: Redirecting stderr to file /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr_train.logrank1
12/09/2017 11:50:54: Redirecting stderr to file /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr_train.logrank2
12/09/2017 11:50:54: Redirecting stderr to file /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr_train.logrank3
[4bb8be993ee1:37524] 3 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[4bb8be993ee1:37524] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
MPI Rank 0: CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:50:53
MPI Rank 0: 
MPI Rank 0: /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 12/09/2017 11:50:53: -------------------------------------------------------------------
MPI Rank 0: 12/09/2017 11:50:53: Build info: 
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:50:53: 		Built time: Dec  8 2017 01:46:20
MPI Rank 0: 12/09/2017 11:50:53: 		Last modified date: Wed Nov 15 09:27:10 2017
MPI Rank 0: 12/09/2017 11:50:53: 		Build type: release
MPI Rank 0: 12/09/2017 11:50:53: 		Build target: GPU
MPI Rank 0: 12/09/2017 11:50:53: 		With 1bit-SGD: yes
MPI Rank 0: 12/09/2017 11:50:53: 		With ASGD: yes
MPI Rank 0: 12/09/2017 11:50:53: 		Math lib: mkl
MPI Rank 0: 12/09/2017 11:50:53: 		CUDA version: 9.0.0
MPI Rank 0: 12/09/2017 11:50:53: 		CUDNN version: 7.0.4
MPI Rank 0: 12/09/2017 11:50:53: 		Build Branch: HEAD
MPI Rank 0: 12/09/2017 11:50:53: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
MPI Rank 0: 12/09/2017 11:50:53: 		MPI distribution: Open MPI
MPI Rank 0: 12/09/2017 11:50:53: 		MPI version: 1.10.7
MPI Rank 0: 12/09/2017 11:50:53: -------------------------------------------------------------------
MPI Rank 0: 12/09/2017 11:50:53: -------------------------------------------------------------------
MPI Rank 0: 12/09/2017 11:50:53: GPU info:
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:50:53: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 8112 MB
MPI Rank 0: 12/09/2017 11:50:53: -------------------------------------------------------------------
MPI Rank 0: 12/09/2017 11:50:53: Using 3 CPU threads.
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:50:53: ##############################################################################
MPI Rank 0: 12/09/2017 11:50:53: #                                                                            #
MPI Rank 0: 12/09/2017 11:50:53: # train command (train action)                                               #
MPI Rank 0: 12/09/2017 11:50:53: #                                                                            #
MPI Rank 0: 12/09/2017 11:50:53: ##############################################################################
MPI Rank 0: 
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 12/09/2017 11:50:53: 
MPI Rank 0: Creating virgin network.
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: 12/09/2017 11:50:53: 
MPI Rank 0: Model has 21 nodes. Using GPU 0.
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:50:53: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing: Out of 36 matrices, 23 are shared as 7, and 13 are not shared.
MPI Rank 0: 
MPI Rank 0: Here are the ones that share memory:
MPI Rank 0: 	{ WD0_D_Tanh : [288 x *]
MPI Rank 0: 	  WQ1_Q : [64 x *] (gradient) }
MPI Rank 0: 	{ SIM : [51 x *] (gradient)
MPI Rank 0: 	  WD0 : [288 x 49292] (gradient)
MPI Rank 0: 	  WD1_D : [64 x *] (gradient) }
MPI Rank 0: 	{ SIM_Scale : [51 x 1 x *]
MPI Rank 0: 	  WD0_D_Tanh : [288 x *] (gradient)
MPI Rank 0: 	  WD1_D_Tanh : [64 x *] (gradient)
MPI Rank 0: 	  WQ0_Q : [288 x *] (gradient) }
MPI Rank 0: 	{ SIM_Scale : [51 x 1 x *] (gradient)
MPI Rank 0: 	  WD1_D : [64 x *]
MPI Rank 0: 	  WQ1 : [64 x 288] (gradient)
MPI Rank 0: 	  WQ1_Q : [64 x *]
MPI Rank 0: 	  WQ1_Q_Tanh : [64 x *] (gradient) }
MPI Rank 0: 	{ SIM : [51 x *]
MPI Rank 0: 	  WD1 : [64 x 288] (gradient) }
MPI Rank 0: 	{ WD0_D : [288 x *]
MPI Rank 0: 	  WD0_D : [288 x *] (gradient)
MPI Rank 0: 	  WD1_D_Tanh : [64 x *]
MPI Rank 0: 	  WQ0_Q : [288 x *]
MPI Rank 0: 	  WQ0_Q_Tanh : [288 x *] (gradient) }
MPI Rank 0: 	{ WQ0 : [288 x 49292] (gradient)
MPI Rank 0: 	  WQ0_Q_Tanh : [288 x *] }
MPI Rank 0: 
MPI Rank 0: Here are the ones that don't share memory:
MPI Rank 0: 	{WQ0 : [288 x 49292]}
MPI Rank 0: 	{Query : [49292 x *]}
MPI Rank 0: 	{Keyword : [49292 x *]}
MPI Rank 0: 	{S : [1 x 1]}
MPI Rank 0: 	{N : [1 x 1]}
MPI Rank 0: 	{G : [1 x 1]}
MPI Rank 0: 	{DSSMLabel : [51 x 1 x *]}
MPI Rank 0: 	{WQ1 : [64 x 288]}
MPI Rank 0: 	{WD0 : [288 x 49292]}
MPI Rank 0: 	{WD1 : [64 x 288]}
MPI Rank 0: 	{ce : [1]}
MPI Rank 0: 	{ce : [1] (gradient)}
MPI Rank 0: 	{WQ1_Q_Tanh : [64 x *]}
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:50:53: Training 28429056 parameters in 4 out of 4 parameter tensors and 15 nodes with gradient:
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:50:53: 	Node 'WD0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 0: 12/09/2017 11:50:53: 	Node 'WD1' (LearnableParameter operation) : [64 x 288]
MPI Rank 0: 12/09/2017 11:50:53: 	Node 'WQ0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 0: 12/09/2017 11:50:53: 	Node 'WQ1' (LearnableParameter operation) : [64 x 288]
MPI Rank 0: 
MPI Rank 0: NcclComm: disabled, same device used by more than one rank
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 12/09/2017 11:50:55: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:50:56: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:50:56: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:50:59:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.34696808 * 10240; time = 3.0994s; samplesPerSecond = 3303.9
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.34277344 * 10240; time = 2.8474s; samplesPerSecond = 3596.3
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:03: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=7.46568s
MPI Rank 0: NcclComm: disabled, same device used by more than one rank
MPI Rank 0: 12/09/2017 11:51:03: Final Results: Minibatch[1-26]: ce = 2.49916008 * 102399; perplexity = 12.17226592
MPI Rank 0: 12/09/2017 11:51:03: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916008 * 102399
MPI Rank 0: 12/09/2017 11:51:04: SGD: Saving checkpoint model '/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:05: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:08:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.30270958 * 10240; time = 2.8653s; samplesPerSecond = 3573.8
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:11:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.09883842 * 10240; time = 2.8590s; samplesPerSecond = 3581.7
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:12: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=7.15861s
MPI Rank 0: NcclComm: disabled, same device used by more than one rank
MPI Rank 0: 12/09/2017 11:51:13: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107634
MPI Rank 0: 12/09/2017 11:51:13: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 0: 12/09/2017 11:51:14: SGD: Saving checkpoint model '/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:15: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:17:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.89778175 * 10240; time = 2.8316s; samplesPerSecond = 3616.3
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:20:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86335983 * 10240; time = 2.8691s; samplesPerSecond = 3569.1
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:22: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=7.13166s
MPI Rank 0: NcclComm: disabled, same device used by more than one rank
MPI Rank 0: 12/09/2017 11:51:22: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 0: 12/09/2017 11:51:22: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 0: 12/09/2017 11:51:23: SGD: Saving checkpoint model '/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/Models/dssm.net'
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:24: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:24: __COMPLETED__
MPI Rank 1: CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:50:53
MPI Rank 1: 
MPI Rank 1: /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 12/09/2017 11:50:53: -------------------------------------------------------------------
MPI Rank 1: 12/09/2017 11:50:53: Build info: 
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:50:53: 		Built time: Dec  8 2017 01:46:20
MPI Rank 1: 12/09/2017 11:50:53: 		Last modified date: Wed Nov 15 09:27:10 2017
MPI Rank 1: 12/09/2017 11:50:53: 		Build type: release
MPI Rank 1: 12/09/2017 11:50:53: 		Build target: GPU
MPI Rank 1: 12/09/2017 11:50:53: 		With 1bit-SGD: yes
MPI Rank 1: 12/09/2017 11:50:53: 		With ASGD: yes
MPI Rank 1: 12/09/2017 11:50:53: 		Math lib: mkl
MPI Rank 1: 12/09/2017 11:50:53: 		CUDA version: 9.0.0
MPI Rank 1: 12/09/2017 11:50:53: 		CUDNN version: 7.0.4
MPI Rank 1: 12/09/2017 11:50:53: 		Build Branch: HEAD
MPI Rank 1: 12/09/2017 11:50:53: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
MPI Rank 1: 12/09/2017 11:50:53: 		MPI distribution: Open MPI
MPI Rank 1: 12/09/2017 11:50:53: 		MPI version: 1.10.7
MPI Rank 1: 12/09/2017 11:50:53: -------------------------------------------------------------------
MPI Rank 1: 12/09/2017 11:50:53: -------------------------------------------------------------------
MPI Rank 1: 12/09/2017 11:50:53: GPU info:
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:50:53: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 7809 MB
MPI Rank 1: 12/09/2017 11:50:53: -------------------------------------------------------------------
MPI Rank 1: 12/09/2017 11:50:53: Using 3 CPU threads.
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:50:53: ##############################################################################
MPI Rank 1: 12/09/2017 11:50:53: #                                                                            #
MPI Rank 1: 12/09/2017 11:50:53: # train command (train action)                                               #
MPI Rank 1: 12/09/2017 11:50:53: #                                                                            #
MPI Rank 1: 12/09/2017 11:50:53: ##############################################################################
MPI Rank 1: 
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 12/09/2017 11:50:53: 
MPI Rank 1: Creating virgin network.
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: 12/09/2017 11:50:54: 
MPI Rank 1: Model has 21 nodes. Using GPU 0.
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:50:54: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing: Out of 36 matrices, 23 are shared as 7, and 13 are not shared.
MPI Rank 1: 
MPI Rank 1: Here are the ones that share memory:
MPI Rank 1: 	{ SIM : [51 x *] (gradient)
MPI Rank 1: 	  WD0 : [288 x 49292] (gradient)
MPI Rank 1: 	  WD1_D : [64 x *] (gradient) }
MPI Rank 1: 	{ SIM : [51 x *]
MPI Rank 1: 	  WD1 : [64 x 288] (gradient) }
MPI Rank 1: 	{ WD0_D_Tanh : [288 x *]
MPI Rank 1: 	  WQ1_Q : [64 x *] (gradient) }
MPI Rank 1: 	{ WQ0 : [288 x 49292] (gradient)
MPI Rank 1: 	  WQ0_Q_Tanh : [288 x *] }
MPI Rank 1: 	{ WD0_D : [288 x *]
MPI Rank 1: 	  WD0_D : [288 x *] (gradient)
MPI Rank 1: 	  WD1_D_Tanh : [64 x *]
MPI Rank 1: 	  WQ0_Q : [288 x *]
MPI Rank 1: 	  WQ0_Q_Tanh : [288 x *] (gradient) }
MPI Rank 1: 	{ SIM_Scale : [51 x 1 x *] (gradient)
MPI Rank 1: 	  WD1_D : [64 x *]
MPI Rank 1: 	  WQ1 : [64 x 288] (gradient)
MPI Rank 1: 	  WQ1_Q : [64 x *]
MPI Rank 1: 	  WQ1_Q_Tanh : [64 x *] (gradient) }
MPI Rank 1: 	{ SIM_Scale : [51 x 1 x *]
MPI Rank 1: 	  WD0_D_Tanh : [288 x *] (gradient)
MPI Rank 1: 	  WD1_D_Tanh : [64 x *] (gradient)
MPI Rank 1: 	  WQ0_Q : [288 x *] (gradient) }
MPI Rank 1: 
MPI Rank 1: Here are the ones that don't share memory:
MPI Rank 1: 	{WQ1_Q_Tanh : [64 x *]}
MPI Rank 1: 	{ce : [1] (gradient)}
MPI Rank 1: 	{ce : [1]}
MPI Rank 1: 	{WD0 : [288 x 49292]}
MPI Rank 1: 	{WD1 : [64 x 288]}
MPI Rank 1: 	{Query : [49292 x *]}
MPI Rank 1: 	{Keyword : [49292 x *]}
MPI Rank 1: 	{S : [1 x 1]}
MPI Rank 1: 	{N : [1 x 1]}
MPI Rank 1: 	{G : [1 x 1]}
MPI Rank 1: 	{DSSMLabel : [51 x 1 x *]}
MPI Rank 1: 	{WQ1 : [64 x 288]}
MPI Rank 1: 	{WQ0 : [288 x 49292]}
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:50:54: Training 28429056 parameters in 4 out of 4 parameter tensors and 15 nodes with gradient:
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:50:54: 	Node 'WD0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 1: 12/09/2017 11:50:54: 	Node 'WD1' (LearnableParameter operation) : [64 x 288]
MPI Rank 1: 12/09/2017 11:50:54: 	Node 'WQ0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 1: 12/09/2017 11:50:54: 	Node 'WQ1' (LearnableParameter operation) : [64 x 288]
MPI Rank 1: 
MPI Rank 1: NcclComm: disabled, same device used by more than one rank
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 12/09/2017 11:50:55: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:50:56: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:50:56: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:50:59:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32159615 * 10240; time = 3.0862s; samplesPerSecond = 3318.0
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.33525505 * 10240; time = 2.8474s; samplesPerSecond = 3596.2
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:03: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=7.46567s
MPI Rank 1: NcclComm: disabled, same device used by more than one rank
MPI Rank 1: 12/09/2017 11:51:03: Final Results: Minibatch[1-26]: ce = 2.49916008 * 102399; perplexity = 12.17226592
MPI Rank 1: 12/09/2017 11:51:03: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916008 * 102399
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:05: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:08:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32732925 * 10240; time = 2.8636s; samplesPerSecond = 3575.9
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:11:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11035995 * 10240; time = 2.8590s; samplesPerSecond = 3581.7
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:12: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=7.15859s
MPI Rank 1: NcclComm: disabled, same device used by more than one rank
MPI Rank 1: 12/09/2017 11:51:13: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107634
MPI Rank 1: 12/09/2017 11:51:13: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:15: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:17:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.92909813 * 10240; time = 2.8316s; samplesPerSecond = 3616.3
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:20:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86598778 * 10240; time = 2.8691s; samplesPerSecond = 3569.1
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:22: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=7.13149s
MPI Rank 1: NcclComm: disabled, same device used by more than one rank
MPI Rank 1: 12/09/2017 11:51:22: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 1: 12/09/2017 11:51:22: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:24: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:24: __COMPLETED__
MPI Rank 2: CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:50:53
MPI Rank 2: 
MPI Rank 2: /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 12/09/2017 11:50:54: -------------------------------------------------------------------
MPI Rank 2: 12/09/2017 11:50:54: Build info: 
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:50:54: 		Built time: Dec  8 2017 01:46:20
MPI Rank 2: 12/09/2017 11:50:54: 		Last modified date: Wed Nov 15 09:27:10 2017
MPI Rank 2: 12/09/2017 11:50:54: 		Build type: release
MPI Rank 2: 12/09/2017 11:50:54: 		Build target: GPU
MPI Rank 2: 12/09/2017 11:50:54: 		With 1bit-SGD: yes
MPI Rank 2: 12/09/2017 11:50:54: 		With ASGD: yes
MPI Rank 2: 12/09/2017 11:50:54: 		Math lib: mkl
MPI Rank 2: 12/09/2017 11:50:54: 		CUDA version: 9.0.0
MPI Rank 2: 12/09/2017 11:50:54: 		CUDNN version: 7.0.4
MPI Rank 2: 12/09/2017 11:50:54: 		Build Branch: HEAD
MPI Rank 2: 12/09/2017 11:50:54: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
MPI Rank 2: 12/09/2017 11:50:54: 		MPI distribution: Open MPI
MPI Rank 2: 12/09/2017 11:50:54: 		MPI version: 1.10.7
MPI Rank 2: 12/09/2017 11:50:54: -------------------------------------------------------------------
MPI Rank 2: 12/09/2017 11:50:54: -------------------------------------------------------------------
MPI Rank 2: 12/09/2017 11:50:54: GPU info:
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:50:54: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 7507 MB
MPI Rank 2: 12/09/2017 11:50:54: -------------------------------------------------------------------
MPI Rank 2: 12/09/2017 11:50:54: Using 3 CPU threads.
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:50:54: ##############################################################################
MPI Rank 2: 12/09/2017 11:50:54: #                                                                            #
MPI Rank 2: 12/09/2017 11:50:54: # train command (train action)                                               #
MPI Rank 2: 12/09/2017 11:50:54: #                                                                            #
MPI Rank 2: 12/09/2017 11:50:54: ##############################################################################
MPI Rank 2: 
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 12/09/2017 11:50:54: 
MPI Rank 2: Creating virgin network.
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: 12/09/2017 11:50:54: 
MPI Rank 2: Model has 21 nodes. Using GPU 0.
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:50:54: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing: Out of 36 matrices, 23 are shared as 7, and 13 are not shared.
MPI Rank 2: 
MPI Rank 2: Here are the ones that share memory:
MPI Rank 2: 	{ WD0_D_Tanh : [288 x *]
MPI Rank 2: 	  WQ1_Q : [64 x *] (gradient) }
MPI Rank 2: 	{ SIM_Scale : [51 x 1 x *] (gradient)
MPI Rank 2: 	  WD1_D : [64 x *]
MPI Rank 2: 	  WQ1 : [64 x 288] (gradient)
MPI Rank 2: 	  WQ1_Q : [64 x *]
MPI Rank 2: 	  WQ1_Q_Tanh : [64 x *] (gradient) }
MPI Rank 2: 	{ SIM : [51 x *] (gradient)
MPI Rank 2: 	  WD0 : [288 x 49292] (gradient)
MPI Rank 2: 	  WD1_D : [64 x *] (gradient) }
MPI Rank 2: 	{ SIM_Scale : [51 x 1 x *]
MPI Rank 2: 	  WD0_D_Tanh : [288 x *] (gradient)
MPI Rank 2: 	  WD1_D_Tanh : [64 x *] (gradient)
MPI Rank 2: 	  WQ0_Q : [288 x *] (gradient) }
MPI Rank 2: 	{ WQ0 : [288 x 49292] (gradient)
MPI Rank 2: 	  WQ0_Q_Tanh : [288 x *] }
MPI Rank 2: 	{ SIM : [51 x *]
MPI Rank 2: 	  WD1 : [64 x 288] (gradient) }
MPI Rank 2: 	{ WD0_D : [288 x *]
MPI Rank 2: 	  WD0_D : [288 x *] (gradient)
MPI Rank 2: 	  WD1_D_Tanh : [64 x *]
MPI Rank 2: 	  WQ0_Q : [288 x *]
MPI Rank 2: 	  WQ0_Q_Tanh : [288 x *] (gradient) }
MPI Rank 2: 
MPI Rank 2: Here are the ones that don't share memory:
MPI Rank 2: 	{WQ0 : [288 x 49292]}
MPI Rank 2: 	{WQ1 : [64 x 288]}
MPI Rank 2: 	{WD0 : [288 x 49292]}
MPI Rank 2: 	{WD1 : [64 x 288]}
MPI Rank 2: 	{Query : [49292 x *]}
MPI Rank 2: 	{Keyword : [49292 x *]}
MPI Rank 2: 	{S : [1 x 1]}
MPI Rank 2: 	{N : [1 x 1]}
MPI Rank 2: 	{G : [1 x 1]}
MPI Rank 2: 	{DSSMLabel : [51 x 1 x *]}
MPI Rank 2: 	{ce : [1]}
MPI Rank 2: 	{ce : [1] (gradient)}
MPI Rank 2: 	{WQ1_Q_Tanh : [64 x *]}
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:50:54: Training 28429056 parameters in 4 out of 4 parameter tensors and 15 nodes with gradient:
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:50:54: 	Node 'WD0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 2: 12/09/2017 11:50:54: 	Node 'WD1' (LearnableParameter operation) : [64 x 288]
MPI Rank 2: 12/09/2017 11:50:54: 	Node 'WQ0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 2: 12/09/2017 11:50:54: 	Node 'WQ1' (LearnableParameter operation) : [64 x 288]
MPI Rank 2: 
MPI Rank 2: NcclComm: disabled, same device used by more than one rank
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 12/09/2017 11:50:55: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:50:56: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:50:56: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:50:59:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32837486 * 10240; time = 3.0886s; samplesPerSecond = 3315.4
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35655556 * 10240; time = 2.8474s; samplesPerSecond = 3596.2
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:03: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=7.46568s
MPI Rank 2: NcclComm: disabled, same device used by more than one rank
MPI Rank 2: 12/09/2017 11:51:03: Final Results: Minibatch[1-26]: ce = 2.49916008 * 102399; perplexity = 12.17226592
MPI Rank 2: 12/09/2017 11:51:03: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916008 * 102399
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:05: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:08:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32893581 * 10240; time = 2.8631s; samplesPerSecond = 3576.6
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:11:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11646900 * 10240; time = 2.8590s; samplesPerSecond = 3581.7
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:12: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=7.15858s
MPI Rank 2: NcclComm: disabled, same device used by more than one rank
MPI Rank 2: 12/09/2017 11:51:13: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107634
MPI Rank 2: 12/09/2017 11:51:13: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:15: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:17:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95308418 * 10240; time = 2.8341s; samplesPerSecond = 3613.1
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:20:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87902641 * 10240; time = 2.8691s; samplesPerSecond = 3569.0
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:22: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=7.13164s
MPI Rank 2: NcclComm: disabled, same device used by more than one rank
MPI Rank 2: 12/09/2017 11:51:22: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 2: 12/09/2017 11:51:22: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:24: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:24: __COMPLETED__
MPI Rank 3: CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:50:53
MPI Rank 3: 
MPI Rank 3: /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 12/09/2017 11:50:54: -------------------------------------------------------------------
MPI Rank 3: 12/09/2017 11:50:54: Build info: 
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:50:54: 		Built time: Dec  8 2017 01:46:20
MPI Rank 3: 12/09/2017 11:50:54: 		Last modified date: Wed Nov 15 09:27:10 2017
MPI Rank 3: 12/09/2017 11:50:54: 		Build type: release
MPI Rank 3: 12/09/2017 11:50:54: 		Build target: GPU
MPI Rank 3: 12/09/2017 11:50:54: 		With 1bit-SGD: yes
MPI Rank 3: 12/09/2017 11:50:54: 		With ASGD: yes
MPI Rank 3: 12/09/2017 11:50:54: 		Math lib: mkl
MPI Rank 3: 12/09/2017 11:50:54: 		CUDA version: 9.0.0
MPI Rank 3: 12/09/2017 11:50:54: 		CUDNN version: 7.0.4
MPI Rank 3: 12/09/2017 11:50:54: 		Build Branch: HEAD
MPI Rank 3: 12/09/2017 11:50:54: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
MPI Rank 3: 12/09/2017 11:50:54: 		MPI distribution: Open MPI
MPI Rank 3: 12/09/2017 11:50:54: 		MPI version: 1.10.7
MPI Rank 3: 12/09/2017 11:50:54: -------------------------------------------------------------------
MPI Rank 3: 12/09/2017 11:50:54: -------------------------------------------------------------------
MPI Rank 3: 12/09/2017 11:50:54: GPU info:
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:50:54: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 7204 MB
MPI Rank 3: 12/09/2017 11:50:54: -------------------------------------------------------------------
MPI Rank 3: 12/09/2017 11:50:54: Using 3 CPU threads.
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:50:54: ##############################################################################
MPI Rank 3: 12/09/2017 11:50:54: #                                                                            #
MPI Rank 3: 12/09/2017 11:50:54: # train command (train action)                                               #
MPI Rank 3: 12/09/2017 11:50:54: #                                                                            #
MPI Rank 3: 12/09/2017 11:50:54: ##############################################################################
MPI Rank 3: 
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 12/09/2017 11:50:54: 
MPI Rank 3: Creating virgin network.
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: 12/09/2017 11:50:55: 
MPI Rank 3: Model has 21 nodes. Using GPU 0.
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:50:55: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing: Out of 36 matrices, 23 are shared as 7, and 13 are not shared.
MPI Rank 3: 
MPI Rank 3: Here are the ones that share memory:
MPI Rank 3: 	{ WD0_D_Tanh : [288 x *]
MPI Rank 3: 	  WQ1_Q : [64 x *] (gradient) }
MPI Rank 3: 	{ SIM_Scale : [51 x 1 x *]
MPI Rank 3: 	  WD0_D_Tanh : [288 x *] (gradient)
MPI Rank 3: 	  WD1_D_Tanh : [64 x *] (gradient)
MPI Rank 3: 	  WQ0_Q : [288 x *] (gradient) }
MPI Rank 3: 	{ SIM_Scale : [51 x 1 x *] (gradient)
MPI Rank 3: 	  WD1_D : [64 x *]
MPI Rank 3: 	  WQ1 : [64 x 288] (gradient)
MPI Rank 3: 	  WQ1_Q : [64 x *]
MPI Rank 3: 	  WQ1_Q_Tanh : [64 x *] (gradient) }
MPI Rank 3: 	{ SIM : [51 x *] (gradient)
MPI Rank 3: 	  WD0 : [288 x 49292] (gradient)
MPI Rank 3: 	  WD1_D : [64 x *] (gradient) }
MPI Rank 3: 	{ SIM : [51 x *]
MPI Rank 3: 	  WD1 : [64 x 288] (gradient) }
MPI Rank 3: 	{ WD0_D : [288 x *]
MPI Rank 3: 	  WD0_D : [288 x *] (gradient)
MPI Rank 3: 	  WD1_D_Tanh : [64 x *]
MPI Rank 3: 	  WQ0_Q : [288 x *]
MPI Rank 3: 	  WQ0_Q_Tanh : [288 x *] (gradient) }
MPI Rank 3: 	{ WQ0 : [288 x 49292] (gradient)
MPI Rank 3: 	  WQ0_Q_Tanh : [288 x *] }
MPI Rank 3: 
MPI Rank 3: Here are the ones that don't share memory:
MPI Rank 3: 	{ce : [1]}
MPI Rank 3: 	{WQ1_Q_Tanh : [64 x *]}
MPI Rank 3: 	{ce : [1] (gradient)}
MPI Rank 3: 	{WQ1 : [64 x 288]}
MPI Rank 3: 	{WD0 : [288 x 49292]}
MPI Rank 3: 	{WD1 : [64 x 288]}
MPI Rank 3: 	{Query : [49292 x *]}
MPI Rank 3: 	{Keyword : [49292 x *]}
MPI Rank 3: 	{S : [1 x 1]}
MPI Rank 3: 	{N : [1 x 1]}
MPI Rank 3: 	{G : [1 x 1]}
MPI Rank 3: 	{DSSMLabel : [51 x 1 x *]}
MPI Rank 3: 	{WQ0 : [288 x 49292]}
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:50:55: Training 28429056 parameters in 4 out of 4 parameter tensors and 15 nodes with gradient:
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:50:55: 	Node 'WD0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 3: 12/09/2017 11:50:55: 	Node 'WD1' (LearnableParameter operation) : [64 x 288]
MPI Rank 3: 12/09/2017 11:50:55: 	Node 'WQ0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 3: 12/09/2017 11:50:55: 	Node 'WQ1' (LearnableParameter operation) : [64 x 288]
MPI Rank 3: 
MPI Rank 3: NcclComm: disabled, same device used by more than one rank
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 12/09/2017 11:50:55: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:50:56: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:50:56: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:50:59:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32287788 * 10240; time = 3.1011s; samplesPerSecond = 3302.0
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35470390 * 10240; time = 2.8474s; samplesPerSecond = 3596.2
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:03: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=7.46566s
MPI Rank 3: NcclComm: disabled, same device used by more than one rank
MPI Rank 3: 12/09/2017 11:51:03: Final Results: Minibatch[1-26]: ce = 2.49916008 * 102399; perplexity = 12.17226592
MPI Rank 3: 12/09/2017 11:51:03: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916008 * 102399
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:05: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:08:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29653873 * 10240; time = 2.8636s; samplesPerSecond = 3575.9
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:11:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11679478 * 10240; time = 2.8590s; samplesPerSecond = 3581.6
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:12: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=7.15858s
MPI Rank 3: NcclComm: disabled, same device used by more than one rank
MPI Rank 3: 12/09/2017 11:51:13: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107634
MPI Rank 3: 12/09/2017 11:51:13: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:15: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:17:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90347176 * 10240; time = 2.8313s; samplesPerSecond = 3616.7
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:20:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88304100 * 10240; time = 2.8691s; samplesPerSecond = 3569.0
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:22: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=7.13148s
MPI Rank 3: NcclComm: disabled, same device used by more than one rank
MPI Rank 3: 12/09/2017 11:51:22: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 3: 12/09/2017 11:51:22: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:24: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:24: __COMPLETED__
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running mpiexec -n 4 /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu DeviceId=0 timestamping=true numCPUThreads=3 stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:51:25

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:51:25

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:51:25

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:51:25

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
Changed current directory to /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData
Changed current directory to /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData
Changed current directory to /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData
Changed current directory to /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData
--------------------------------------------------------------------------
[[4674,1],3]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: 4bb8be993ee1

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (3) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (1) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (2) are in (participating)
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (0) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
12/09/2017 11:51:25: Redirecting stderr to file /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr_train.logrank0
12/09/2017 11:51:26: Redirecting stderr to file /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr_train.logrank1
12/09/2017 11:51:26: Redirecting stderr to file /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr_train.logrank2
12/09/2017 11:51:27: Redirecting stderr to file /tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr_train.logrank3
[4bb8be993ee1:38224] 3 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[4bb8be993ee1:38224] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
MPI Rank 0: CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:51:25
MPI Rank 0: 
MPI Rank 0: /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 12/09/2017 11:51:25: -------------------------------------------------------------------
MPI Rank 0: 12/09/2017 11:51:25: Build info: 
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:25: 		Built time: Dec  8 2017 01:46:20
MPI Rank 0: 12/09/2017 11:51:25: 		Last modified date: Wed Nov 15 09:27:10 2017
MPI Rank 0: 12/09/2017 11:51:25: 		Build type: release
MPI Rank 0: 12/09/2017 11:51:25: 		Build target: GPU
MPI Rank 0: 12/09/2017 11:51:25: 		With 1bit-SGD: yes
MPI Rank 0: 12/09/2017 11:51:25: 		With ASGD: yes
MPI Rank 0: 12/09/2017 11:51:25: 		Math lib: mkl
MPI Rank 0: 12/09/2017 11:51:25: 		CUDA version: 9.0.0
MPI Rank 0: 12/09/2017 11:51:25: 		CUDNN version: 7.0.4
MPI Rank 0: 12/09/2017 11:51:25: 		Build Branch: HEAD
MPI Rank 0: 12/09/2017 11:51:25: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
MPI Rank 0: 12/09/2017 11:51:25: 		MPI distribution: Open MPI
MPI Rank 0: 12/09/2017 11:51:25: 		MPI version: 1.10.7
MPI Rank 0: 12/09/2017 11:51:25: -------------------------------------------------------------------
MPI Rank 0: 12/09/2017 11:51:25: -------------------------------------------------------------------
MPI Rank 0: 12/09/2017 11:51:25: GPU info:
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:25: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 8112 MB
MPI Rank 0: 12/09/2017 11:51:25: -------------------------------------------------------------------
MPI Rank 0: 12/09/2017 11:51:25: Using 3 CPU threads.
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:25: ##############################################################################
MPI Rank 0: 12/09/2017 11:51:25: #                                                                            #
MPI Rank 0: 12/09/2017 11:51:25: # train command (train action)                                               #
MPI Rank 0: 12/09/2017 11:51:25: #                                                                            #
MPI Rank 0: 12/09/2017 11:51:25: ##############################################################################
MPI Rank 0: 
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 12/09/2017 11:51:25: 
MPI Rank 0: Starting from checkpoint. Loading network from '/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: 12/09/2017 11:51:27: 
MPI Rank 0: Model has 21 nodes. Using GPU 0.
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:27: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:27: Training 28429056 parameters in 4 out of 4 parameter tensors and 15 nodes with gradient:
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:27: 	Node 'WD0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 0: 12/09/2017 11:51:27: 	Node 'WD1' (LearnableParameter operation) : [64 x 288]
MPI Rank 0: 12/09/2017 11:51:27: 	Node 'WQ0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 0: 12/09/2017 11:51:27: 	Node 'WQ1' (LearnableParameter operation) : [64 x 288]
MPI Rank 0: 
MPI Rank 0: NcclComm: disabled, same device used by more than one rank
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 12/09/2017 11:51:28: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:29: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:29: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:32:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87577038 * 10240; time = 3.0963s; samplesPerSecond = 3307.1
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:35:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79361134 * 10240; time = 2.9085s; samplesPerSecond = 3520.7
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 0: 12/09/2017 11:51:37: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=7.54093s
MPI Rank 0: NcclComm: disabled, same device used by more than one rank
MPI Rank 0: 12/09/2017 11:51:37: Final Results: Minibatch[1-26]: ce = 1.81846900 * 102399; perplexity = 6.16241656
MPI Rank 0: 12/09/2017 11:51:37: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846900 * 102399
MPI Rank 0: 12/09/2017 11:51:38: SGD: Saving checkpoint model '/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/Models/dssm.net'
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:39: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 12/09/2017 11:51:39: __COMPLETED__
MPI Rank 1: CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:51:25
MPI Rank 1: 
MPI Rank 1: /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 12/09/2017 11:51:26: -------------------------------------------------------------------
MPI Rank 1: 12/09/2017 11:51:26: Build info: 
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:26: 		Built time: Dec  8 2017 01:46:20
MPI Rank 1: 12/09/2017 11:51:26: 		Last modified date: Wed Nov 15 09:27:10 2017
MPI Rank 1: 12/09/2017 11:51:26: 		Build type: release
MPI Rank 1: 12/09/2017 11:51:26: 		Build target: GPU
MPI Rank 1: 12/09/2017 11:51:26: 		With 1bit-SGD: yes
MPI Rank 1: 12/09/2017 11:51:26: 		With ASGD: yes
MPI Rank 1: 12/09/2017 11:51:26: 		Math lib: mkl
MPI Rank 1: 12/09/2017 11:51:26: 		CUDA version: 9.0.0
MPI Rank 1: 12/09/2017 11:51:26: 		CUDNN version: 7.0.4
MPI Rank 1: 12/09/2017 11:51:26: 		Build Branch: HEAD
MPI Rank 1: 12/09/2017 11:51:26: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
MPI Rank 1: 12/09/2017 11:51:26: 		MPI distribution: Open MPI
MPI Rank 1: 12/09/2017 11:51:26: 		MPI version: 1.10.7
MPI Rank 1: 12/09/2017 11:51:26: -------------------------------------------------------------------
MPI Rank 1: 12/09/2017 11:51:26: -------------------------------------------------------------------
MPI Rank 1: 12/09/2017 11:51:26: GPU info:
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:26: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 8027 MB
MPI Rank 1: 12/09/2017 11:51:26: -------------------------------------------------------------------
MPI Rank 1: 12/09/2017 11:51:26: Using 3 CPU threads.
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:26: ##############################################################################
MPI Rank 1: 12/09/2017 11:51:26: #                                                                            #
MPI Rank 1: 12/09/2017 11:51:26: # train command (train action)                                               #
MPI Rank 1: 12/09/2017 11:51:26: #                                                                            #
MPI Rank 1: 12/09/2017 11:51:26: ##############################################################################
MPI Rank 1: 
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 12/09/2017 11:51:26: 
MPI Rank 1: Starting from checkpoint. Loading network from '/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: 12/09/2017 11:51:27: 
MPI Rank 1: Model has 21 nodes. Using GPU 0.
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:27: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:27: Training 28429056 parameters in 4 out of 4 parameter tensors and 15 nodes with gradient:
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:27: 	Node 'WD0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 1: 12/09/2017 11:51:27: 	Node 'WD1' (LearnableParameter operation) : [64 x 288]
MPI Rank 1: 12/09/2017 11:51:27: 	Node 'WQ0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 1: 12/09/2017 11:51:27: 	Node 'WQ1' (LearnableParameter operation) : [64 x 288]
MPI Rank 1: 
MPI Rank 1: NcclComm: disabled, same device used by more than one rank
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 12/09/2017 11:51:28: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:29: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:29: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 12/09/2017 11:51:32:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93745022 * 10240; time = 3.1244s; samplesPerSecond = 3277.5
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:35:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89571209 * 10240; time = 2.9085s; samplesPerSecond = 3520.7
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 1: 12/09/2017 11:51:37: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=7.54092s
MPI Rank 1: NcclComm: disabled, same device used by more than one rank
MPI Rank 1: 12/09/2017 11:51:37: Final Results: Minibatch[1-26]: ce = 1.81846900 * 102399; perplexity = 6.16241656
MPI Rank 1: 12/09/2017 11:51:37: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846900 * 102399
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:39: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 12/09/2017 11:51:39: __COMPLETED__
MPI Rank 2: CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:51:25
MPI Rank 2: 
MPI Rank 2: /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 12/09/2017 11:51:26: -------------------------------------------------------------------
MPI Rank 2: 12/09/2017 11:51:26: Build info: 
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:26: 		Built time: Dec  8 2017 01:46:20
MPI Rank 2: 12/09/2017 11:51:26: 		Last modified date: Wed Nov 15 09:27:10 2017
MPI Rank 2: 12/09/2017 11:51:26: 		Build type: release
MPI Rank 2: 12/09/2017 11:51:26: 		Build target: GPU
MPI Rank 2: 12/09/2017 11:51:26: 		With 1bit-SGD: yes
MPI Rank 2: 12/09/2017 11:51:26: 		With ASGD: yes
MPI Rank 2: 12/09/2017 11:51:26: 		Math lib: mkl
MPI Rank 2: 12/09/2017 11:51:26: 		CUDA version: 9.0.0
MPI Rank 2: 12/09/2017 11:51:26: 		CUDNN version: 7.0.4
MPI Rank 2: 12/09/2017 11:51:26: 		Build Branch: HEAD
MPI Rank 2: 12/09/2017 11:51:26: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
MPI Rank 2: 12/09/2017 11:51:26: 		MPI distribution: Open MPI
MPI Rank 2: 12/09/2017 11:51:26: 		MPI version: 1.10.7
MPI Rank 2: 12/09/2017 11:51:26: -------------------------------------------------------------------
MPI Rank 2: 12/09/2017 11:51:26: -------------------------------------------------------------------
MPI Rank 2: 12/09/2017 11:51:26: GPU info:
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:26: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 7888 MB
MPI Rank 2: 12/09/2017 11:51:26: -------------------------------------------------------------------
MPI Rank 2: 12/09/2017 11:51:26: Using 3 CPU threads.
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:26: ##############################################################################
MPI Rank 2: 12/09/2017 11:51:26: #                                                                            #
MPI Rank 2: 12/09/2017 11:51:26: # train command (train action)                                               #
MPI Rank 2: 12/09/2017 11:51:26: #                                                                            #
MPI Rank 2: 12/09/2017 11:51:26: ##############################################################################
MPI Rank 2: 
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 12/09/2017 11:51:26: 
MPI Rank 2: Starting from checkpoint. Loading network from '/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: 12/09/2017 11:51:28: 
MPI Rank 2: Model has 21 nodes. Using GPU 0.
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:28: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:28: Training 28429056 parameters in 4 out of 4 parameter tensors and 15 nodes with gradient:
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:28: 	Node 'WD0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 2: 12/09/2017 11:51:28: 	Node 'WD1' (LearnableParameter operation) : [64 x 288]
MPI Rank 2: 12/09/2017 11:51:28: 	Node 'WQ0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 2: 12/09/2017 11:51:28: 	Node 'WQ1' (LearnableParameter operation) : [64 x 288]
MPI Rank 2: 
MPI Rank 2: NcclComm: disabled, same device used by more than one rank
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 12/09/2017 11:51:28: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:29: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:29: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 12/09/2017 11:51:32:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96188030 * 10240; time = 3.1138s; samplesPerSecond = 3288.6
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:35:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90950031 * 10240; time = 2.9085s; samplesPerSecond = 3520.7
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 12/09/2017 11:51:37: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=7.54092s
MPI Rank 2: NcclComm: disabled, same device used by more than one rank
MPI Rank 2: 12/09/2017 11:51:37: Final Results: Minibatch[1-26]: ce = 1.81846900 * 102399; perplexity = 6.16241656
MPI Rank 2: 12/09/2017 11:51:37: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846900 * 102399
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:39: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 12/09/2017 11:51:39: __COMPLETED__
MPI Rank 3: CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:51:25
MPI Rank 3: 
MPI Rank 3: /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=3  stderr=/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 12/09/2017 11:51:27: -------------------------------------------------------------------
MPI Rank 3: 12/09/2017 11:51:27: Build info: 
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:27: 		Built time: Dec  8 2017 01:46:20
MPI Rank 3: 12/09/2017 11:51:27: 		Last modified date: Wed Nov 15 09:27:10 2017
MPI Rank 3: 12/09/2017 11:51:27: 		Build type: release
MPI Rank 3: 12/09/2017 11:51:27: 		Build target: GPU
MPI Rank 3: 12/09/2017 11:51:27: 		With 1bit-SGD: yes
MPI Rank 3: 12/09/2017 11:51:27: 		With ASGD: yes
MPI Rank 3: 12/09/2017 11:51:27: 		Math lib: mkl
MPI Rank 3: 12/09/2017 11:51:27: 		CUDA version: 9.0.0
MPI Rank 3: 12/09/2017 11:51:27: 		CUDNN version: 7.0.4
MPI Rank 3: 12/09/2017 11:51:27: 		Build Branch: HEAD
MPI Rank 3: 12/09/2017 11:51:27: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
MPI Rank 3: 12/09/2017 11:51:27: 		MPI distribution: Open MPI
MPI Rank 3: 12/09/2017 11:51:27: 		MPI version: 1.10.7
MPI Rank 3: 12/09/2017 11:51:27: -------------------------------------------------------------------
MPI Rank 3: 12/09/2017 11:51:27: -------------------------------------------------------------------
MPI Rank 3: 12/09/2017 11:51:27: GPU info:
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:27: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 7587 MB
MPI Rank 3: 12/09/2017 11:51:27: -------------------------------------------------------------------
MPI Rank 3: 12/09/2017 11:51:27: Using 3 CPU threads.
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:27: ##############################################################################
MPI Rank 3: 12/09/2017 11:51:27: #                                                                            #
MPI Rank 3: 12/09/2017 11:51:27: # train command (train action)                                               #
MPI Rank 3: 12/09/2017 11:51:27: #                                                                            #
MPI Rank 3: 12/09/2017 11:51:27: ##############################################################################
MPI Rank 3: 
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 12/09/2017 11:51:27: 
MPI Rank 3: Starting from checkpoint. Loading network from '/tmp/cntk-test-20171209080859.615414/Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: 12/09/2017 11:51:28: 
MPI Rank 3: Model has 21 nodes. Using GPU 0.
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:28: Training criterion:   ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:28: Training 28429056 parameters in 4 out of 4 parameter tensors and 15 nodes with gradient:
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:28: 	Node 'WD0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 3: 12/09/2017 11:51:28: 	Node 'WD1' (LearnableParameter operation) : [64 x 288]
MPI Rank 3: 12/09/2017 11:51:28: 	Node 'WQ0' (LearnableParameter operation) : [288 x 49292]
MPI Rank 3: 12/09/2017 11:51:28: 	Node 'WQ1' (LearnableParameter operation) : [64 x 288]
MPI Rank 3: 
MPI Rank 3: NcclComm: disabled, same device used by more than one rank
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 12/09/2017 11:51:28: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:29: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:29: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 12/09/2017 11:51:32:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91174297 * 10240; time = 3.1256s; samplesPerSecond = 3276.2
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:35:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91370487 * 10240; time = 2.9086s; samplesPerSecond = 3520.6
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 3: 12/09/2017 11:51:37: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=7.54091s
MPI Rank 3: NcclComm: disabled, same device used by more than one rank
MPI Rank 3: 12/09/2017 11:51:37: Final Results: Minibatch[1-26]: ce = 1.81846900 * 102399; perplexity = 6.16241656
MPI Rank 3: 12/09/2017 11:51:37: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846900 * 102399
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:39: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 12/09/2017 11:51:39: __COMPLETED__