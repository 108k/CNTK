CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz
    Hardware threads: 6
    Total Memory: 58719796 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader/cntk_sequence.cntk currentDirectory=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData RunDir=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu DataDir=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader OutputDir=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu DeviceId=0 timestamping=true
CNTK 2.3.1+ (HEAD d81022, Dec 30 2017 00:21:38) at 2017/12/30 01:13:55

C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader/cntk_sequence.cntk  currentDirectory=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData  RunDir=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu  DataDir=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader  OutputDir=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu  DeviceId=0  timestamping=true
Changed current directory to D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData
-------------------------------------------------------------------
Build info: 

		Built time: Dec 30 2017 00:09:24
		Last modified date: Sat Dec 16 09:29:58 2017
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		With ASGD: yes
		Math lib: mkl
		CUDA version: 9.0.0
		CUDNN version: 6.0.21
		Build Branch: HEAD
		Build SHA1: d810222168c9383a216c0bbc815bc8b8351f28b3
		MPI distribution: Microsoft MPI
		MPI version: 7.0.12437.6
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8124 MB; free memory = 8002 MB
-------------------------------------------------------------------

Configuration After Processing and Variable Resolution:

configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader
configparameters: cntk_sequence.cntk:currentDirectory=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData
configparameters: cntk_sequence.cntk:DataDir=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:RunDir=D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech.sequence
    traceLevel = 1
    SGD = [
        epochSize = 6000000
        minibatchSize = 800000
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
		maxEpochs = 3
		gradientClippingWithTruncation = true
		clippingThresholdPerSample = 1.0
    ]
	reader = [
			verbosity = 0
			randomize = false
			deserializers = (
				[
					type = "HTKFeatureDeserializer"
					module = "HTKDeserializers"
					input = [
						features = [
							dim=363
							scpFile = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.scp"
						]
					]
				]:
				[
					type = "HTKMLFDeserializer"
					module = "HTKDeserializers"
					input = [
						labels = [
							dim = 132
							mlfFile="D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.mlf"
							labelMappingFile = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/state.list" 
						]
					]
				]:
				[
					type = "LatticeDeserializer"
					module = "HTKDeserializers"
					input = [
						lattice=[
							latticeIndexFile="D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/latticeIndex.txt"
						]
					]
				]
			)
		]
		BrainScriptNetworkBuilder = {
			baseFeatDim = 33
			featDim = 11 * baseFeatDim
			labelDim = 132
			latticeAxis = DynamicAxis()
			features = Input{featDim}
			labels = Input{labelDim}
			lattice = Input{1,dynamicAxis=latticeAxis}
			featExtNetwork  = BS.Network.Load("D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech")
			featExt = BS.Network.CloneFunction (
              (featExtNetwork.features),
              [netEval = featExtNetwork.OL_z;scaledLogLikelihood = featExtNetwork.scaledLogLikelihood ],
              parameters="learnable")
			clonedmodel= featExt(features)
			cr = SequenceWithLattice(labels, clonedmodel.netEval, clonedmodel.scaledLogLikelihood, lattice, "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/CY2SCH010061231_1369712653.numden.lats.symlist", "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/model.overalltying", "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/state.list", "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/model.transprob", tag="criterion")  
			Err = ClassificationError(labels,clonedmodel.netEval,tag="evaluation");
		}
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTrainingNewReader/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
12/30/2017 01:13:55: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain sequenceTrain
12/30/2017 01:13:55: precision = "float"

12/30/2017 01:13:55: ##############################################################################
12/30/2017 01:13:55: #                                                                            #
12/30/2017 01:13:55: # dptPre1 command (train action)                                             #
12/30/2017 01:13:55: #                                                                            #
12/30/2017 01:13:55: ##############################################################################

12/30/2017 01:13:55: 
Creating virgin network.
NDLBuilder Using GPU 0
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
reading script file D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/state.list
htkmlfreader: reading MLF file D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
12/30/2017 01:13:56: 
Model has 19 nodes. Using GPU 0.

12/30/2017 01:13:56: Training criterion:   ce = CrossEntropyWithSoftmax
12/30/2017 01:13:56: Evaluation criterion: err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 2 are aliased.
	OL.t (gradient) reuses OL.z (gradient)

Memory Sharing: Out of 29 matrices, 11 are shared as 3, and 18 are not shared.

Here are the ones that share memory:
	{ HL1.W : [512 x 363] (gradient)
	  HL1.t : [512 x *]
	  HL1.y : [512 x 1 x *] }
	{ HL1.t : [512 x *] (gradient)
	  HL1.y : [512 x 1 x *] (gradient)
	  HL1.z : [512 x 1 x *]
	  OL.z : [132 x 1 x *] }
	{ HL1.z : [512 x 1 x *] (gradient)
	  OL.t : [132 x 1 x *]
	  OL.t : [132 x 1 x *] (gradient)
	  OL.z : [132 x 1 x *] (gradient) }

Here are the ones that don't share memory:
	{scaledLogLikelihood : [132 x 1 x *]}
	{features : [363 x *]}
	{globalPrior : [132 x 1]}
	{labels : [132 x *]}
	{globalMean : [363 x 1]}
	{globalInvStd : [363 x 1]}
	{HL1.W : [512 x 363]}
	{OL.W : [132 x 512]}
	{OL.b : [132 x 1]}
	{HL1.b : [512 x 1]}
	{ce : [1] (gradient)}
	{featNorm : [363 x *]}
	{ce : [1]}
	{err : [1]}
	{OL.W : [132 x 512] (gradient)}
	{HL1.b : [512 x 1] (gradient)}
	{logPrior : [132 x 1]}
	{OL.b : [132 x 1] (gradient)}


12/30/2017 01:13:56: Training 254084 parameters in 4 out of 4 parameter tensors and 10 nodes with gradient:

12/30/2017 01:13:56: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
12/30/2017 01:13:56: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
12/30/2017 01:13:56: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
12/30/2017 01:13:56: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

12/30/2017 01:13:56: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/30/2017 01:13:56: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

12/30/2017 01:13:56: Starting minibatch loop.
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.89978218 * 2560; err = 0.84375000 * 2560; time = 0.2304s; samplesPerSecond = 11112.2
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.96755714 * 2560; err = 0.72031250 * 2560; time = 0.0086s; samplesPerSecond = 296828.8
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.55723495 * 2560; err = 0.65859375 * 2560; time = 0.0084s; samplesPerSecond = 303828.7
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.29642792 * 2560; err = 0.61992187 * 2560; time = 0.0085s; samplesPerSecond = 300564.7
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 2.02396393 * 2560; err = 0.55117187 * 2560; time = 0.0084s; samplesPerSecond = 304218.7
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.87309265 * 2560; err = 0.51484375 * 2560; time = 0.0082s; samplesPerSecond = 313621.7
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.78157196 * 2560; err = 0.50507813 * 2560; time = 0.0081s; samplesPerSecond = 316918.0
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.75391235 * 2560; err = 0.50781250 * 2560; time = 0.0081s; samplesPerSecond = 317617.9
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.66460266 * 2560; err = 0.45742187 * 2560; time = 0.0080s; samplesPerSecond = 319333.4
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62184296 * 2560; err = 0.47968750 * 2560; time = 0.0079s; samplesPerSecond = 322312.6
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.65328217 * 2560; err = 0.47265625 * 2560; time = 0.0082s; samplesPerSecond = 313852.4
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.50686951 * 2560; err = 0.44921875 * 2560; time = 0.0083s; samplesPerSecond = 309788.6
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.46723938 * 2560; err = 0.42304687 * 2560; time = 0.0080s; samplesPerSecond = 321947.8
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.49163513 * 2560; err = 0.44140625 * 2560; time = 0.0079s; samplesPerSecond = 322767.7
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.46437683 * 2560; err = 0.43398437 * 2560; time = 0.0080s; samplesPerSecond = 320902.5
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.43047791 * 2560; err = 0.43867187 * 2560; time = 0.0080s; samplesPerSecond = 318103.3
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.42104797 * 2560; err = 0.41992188 * 2560; time = 0.0081s; samplesPerSecond = 317283.3
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.46536865 * 2560; err = 0.42421875 * 2560; time = 0.0079s; samplesPerSecond = 324112.2
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.47427673 * 2560; err = 0.44062500 * 2560; time = 0.0079s; samplesPerSecond = 322759.6
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42848816 * 2560; err = 0.44062500 * 2560; time = 0.0079s; samplesPerSecond = 324264.1
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.34078674 * 2560; err = 0.41171875 * 2560; time = 0.0079s; samplesPerSecond = 323776.0
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.39474792 * 2560; err = 0.42773438 * 2560; time = 0.0079s; samplesPerSecond = 323800.6
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.40156250 * 2560; err = 0.41250000 * 2560; time = 0.0091s; samplesPerSecond = 280391.2
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.39350281 * 2560; err = 0.42734375 * 2560; time = 0.0082s; samplesPerSecond = 313015.8
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.32500916 * 2560; err = 0.40156250 * 2560; time = 0.0081s; samplesPerSecond = 315698.6
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27034607 * 2560; err = 0.39804688 * 2560; time = 0.0080s; samplesPerSecond = 318697.3
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.32393494 * 2560; err = 0.39375000 * 2560; time = 0.0081s; samplesPerSecond = 317834.8
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.25440979 * 2560; err = 0.38437500 * 2560; time = 0.0080s; samplesPerSecond = 320705.6
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.23372498 * 2560; err = 0.37148437 * 2560; time = 0.0085s; samplesPerSecond = 300645.9
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20885620 * 2560; err = 0.35976562 * 2560; time = 0.0084s; samplesPerSecond = 304703.9
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23717957 * 2560; err = 0.36953125 * 2560; time = 0.0082s; samplesPerSecond = 312973.7
12/30/2017 01:13:56:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.23036499 * 2560; err = 0.37382813 * 2560; time = 0.0079s; samplesPerSecond = 323649.1
12/30/2017 01:13:56: Finished Epoch[ 1 of 2]: [Training] ce = 1.65179615 * 81920; err = 0.46795654 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.613362s
12/30/2017 01:13:56: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre1/cntkSpeech.1'

12/30/2017 01:13:56: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

12/30/2017 01:13:56: Starting minibatch loop.
12/30/2017 01:13:56:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.21826115 * 2560; err = 0.37031250 * 2560; time = 0.0106s; samplesPerSecond = 240818.8
12/30/2017 01:13:56:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.18394566 * 2560; err = 0.36640625 * 2560; time = 0.0082s; samplesPerSecond = 312977.6
12/30/2017 01:13:56:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.17353325 * 2560; err = 0.35976562 * 2560; time = 0.0081s; samplesPerSecond = 315430.2
12/30/2017 01:13:56:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.20286140 * 2560; err = 0.35859375 * 2560; time = 0.0082s; samplesPerSecond = 313687.0
12/30/2017 01:13:56:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.19606552 * 2560; err = 0.37812500 * 2560; time = 0.0081s; samplesPerSecond = 317886.1
12/30/2017 01:13:56:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.16461678 * 2560; err = 0.34375000 * 2560; time = 0.0082s; samplesPerSecond = 311613.7
12/30/2017 01:13:56:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.13763046 * 2560; err = 0.34765625 * 2560; time = 0.0080s; samplesPerSecond = 320300.3
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.19578094 * 2560; err = 0.37226562 * 2560; time = 0.0080s; samplesPerSecond = 319130.4
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.24385757 * 2560; err = 0.38046875 * 2560; time = 0.0079s; samplesPerSecond = 322292.3
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18975372 * 2560; err = 0.36093750 * 2560; time = 0.0080s; samplesPerSecond = 321329.5
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.16871338 * 2560; err = 0.35625000 * 2560; time = 0.0081s; samplesPerSecond = 316945.4
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.24394531 * 2560; err = 0.37929687 * 2560; time = 0.0081s; samplesPerSecond = 315177.8
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.18284454 * 2560; err = 0.34921875 * 2560; time = 0.0079s; samplesPerSecond = 323416.1
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.21535187 * 2560; err = 0.36875000 * 2560; time = 0.0080s; samplesPerSecond = 321345.6
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.19859619 * 2560; err = 0.37187500 * 2560; time = 0.0079s; samplesPerSecond = 323134.4
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.14699402 * 2560; err = 0.34648438 * 2560; time = 0.0079s; samplesPerSecond = 322199.0
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.14664307 * 2560; err = 0.35859375 * 2560; time = 0.0079s; samplesPerSecond = 324083.5
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.17961731 * 2560; err = 0.35234375 * 2560; time = 0.0085s; samplesPerSecond = 301010.0
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.14800873 * 2560; err = 0.35234375 * 2560; time = 0.0079s; samplesPerSecond = 322422.2
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08513489 * 2560; err = 0.33320312 * 2560; time = 0.0079s; samplesPerSecond = 322113.9
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.14455261 * 2560; err = 0.34726563 * 2560; time = 0.0079s; samplesPerSecond = 323314.0
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.16282043 * 2560; err = 0.35703125 * 2560; time = 0.0080s; samplesPerSecond = 321858.7
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.19267578 * 2560; err = 0.37265625 * 2560; time = 0.0079s; samplesPerSecond = 323579.6
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.15575562 * 2560; err = 0.34882812 * 2560; time = 0.0079s; samplesPerSecond = 323596.0
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.15968018 * 2560; err = 0.35351563 * 2560; time = 0.0079s; samplesPerSecond = 323944.0
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.08686829 * 2560; err = 0.33007813 * 2560; time = 0.0081s; samplesPerSecond = 315799.9
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.10071411 * 2560; err = 0.34804687 * 2560; time = 0.0080s; samplesPerSecond = 318538.7
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.06582336 * 2560; err = 0.33945313 * 2560; time = 0.0079s; samplesPerSecond = 323931.7
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.09620972 * 2560; err = 0.33085938 * 2560; time = 0.0079s; samplesPerSecond = 324893.7
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.15163269 * 2560; err = 0.35195312 * 2560; time = 0.0079s; samplesPerSecond = 324424.3
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.10898132 * 2560; err = 0.34296875 * 2560; time = 0.0081s; samplesPerSecond = 317842.6
12/30/2017 01:13:57:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06808777 * 2560; err = 0.32304688 * 2560; time = 0.0078s; samplesPerSecond = 329807.7
12/30/2017 01:13:57: Finished Epoch[ 2 of 2]: [Training] ce = 1.15987368 * 81920; err = 0.35476074 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.262257s
12/30/2017 01:13:57: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre1/cntkSpeech'

12/30/2017 01:13:57: Action "train" complete.


12/30/2017 01:13:57: ##############################################################################
12/30/2017 01:13:57: #                                                                            #
12/30/2017 01:13:57: # addLayer2 command (edit action)                                            #
12/30/2017 01:13:57: #                                                                            #
12/30/2017 01:13:57: ##############################################################################


12/30/2017 01:13:57: Action "edit" complete.


12/30/2017 01:13:57: ##############################################################################
12/30/2017 01:13:57: #                                                                            #
12/30/2017 01:13:57: # dptPre2 command (train action)                                             #
12/30/2017 01:13:57: #                                                                            #
12/30/2017 01:13:57: ##############################################################################

12/30/2017 01:13:57: 
Starting from checkpoint. Loading network from 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre2/cntkSpeech.0'.
NDLBuilder Using GPU 0
reading script file D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/state.list
htkmlfreader: reading MLF file D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
12/30/2017 01:13:57: 
Model has 24 nodes. Using GPU 0.

12/30/2017 01:13:57: Training criterion:   ce = CrossEntropyWithSoftmax
12/30/2017 01:13:57: Evaluation criterion: err = ClassificationError

12/30/2017 01:13:57: Training 516740 parameters in 6 out of 6 parameter tensors and 15 nodes with gradient:

12/30/2017 01:13:57: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
12/30/2017 01:13:57: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
12/30/2017 01:13:57: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
12/30/2017 01:13:57: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
12/30/2017 01:13:57: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
12/30/2017 01:13:57: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

12/30/2017 01:13:57: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/30/2017 01:13:57: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

12/30/2017 01:13:57: Starting minibatch loop.
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 4.68514671 * 2560; err = 0.80507812 * 2560; time = 0.0188s; samplesPerSecond = 136081.2
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.83540916 * 2560; err = 0.69765625 * 2560; time = 0.0106s; samplesPerSecond = 240870.9
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.27396317 * 2560; err = 0.59335938 * 2560; time = 0.0105s; samplesPerSecond = 242999.5
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.93453598 * 2560; err = 0.52070313 * 2560; time = 0.0105s; samplesPerSecond = 243322.9
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.72010727 * 2560; err = 0.47890625 * 2560; time = 0.0105s; samplesPerSecond = 243101.1
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.61654205 * 2560; err = 0.47070313 * 2560; time = 0.0105s; samplesPerSecond = 242997.2
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.51410522 * 2560; err = 0.44140625 * 2560; time = 0.0108s; samplesPerSecond = 237012.9
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.49882050 * 2560; err = 0.43671875 * 2560; time = 0.0105s; samplesPerSecond = 243278.9
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.45906067 * 2560; err = 0.41835937 * 2560; time = 0.0105s; samplesPerSecond = 242946.5
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41359100 * 2560; err = 0.40937500 * 2560; time = 0.0105s; samplesPerSecond = 242939.6
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.41605682 * 2560; err = 0.41093750 * 2560; time = 0.0107s; samplesPerSecond = 239265.8
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.34621582 * 2560; err = 0.39531250 * 2560; time = 0.0106s; samplesPerSecond = 241495.8
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.32200775 * 2560; err = 0.39257813 * 2560; time = 0.0105s; samplesPerSecond = 243232.7
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.33358459 * 2560; err = 0.39609375 * 2560; time = 0.0106s; samplesPerSecond = 242256.8
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.32798462 * 2560; err = 0.39023438 * 2560; time = 0.0107s; samplesPerSecond = 238930.8
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.28152161 * 2560; err = 0.38906250 * 2560; time = 0.0105s; samplesPerSecond = 242847.4
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.29407654 * 2560; err = 0.38476563 * 2560; time = 0.0105s; samplesPerSecond = 242704.6
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.29927979 * 2560; err = 0.39023438 * 2560; time = 0.0105s; samplesPerSecond = 242771.4
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33316956 * 2560; err = 0.40546875 * 2560; time = 0.0105s; samplesPerSecond = 243230.4
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.32376099 * 2560; err = 0.41406250 * 2560; time = 0.0120s; samplesPerSecond = 212831.4
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.23729553 * 2560; err = 0.37539062 * 2560; time = 0.0107s; samplesPerSecond = 238986.5
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.27207336 * 2560; err = 0.38906250 * 2560; time = 0.0107s; samplesPerSecond = 240316.9
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.26402893 * 2560; err = 0.37695313 * 2560; time = 0.0105s; samplesPerSecond = 242716.1
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.24779053 * 2560; err = 0.36992188 * 2560; time = 0.0106s; samplesPerSecond = 242348.5
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.21864014 * 2560; err = 0.36757812 * 2560; time = 0.0105s; samplesPerSecond = 242718.4
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.19121704 * 2560; err = 0.37070313 * 2560; time = 0.0108s; samplesPerSecond = 236140.6
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23925476 * 2560; err = 0.36835937 * 2560; time = 0.0105s; samplesPerSecond = 242870.4
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.18916016 * 2560; err = 0.36406250 * 2560; time = 0.0105s; samplesPerSecond = 242739.2
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.17012329 * 2560; err = 0.35351563 * 2560; time = 0.0105s; samplesPerSecond = 242810.5
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.14500122 * 2560; err = 0.33984375 * 2560; time = 0.0106s; samplesPerSecond = 242635.6
12/30/2017 01:13:57:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.17140808 * 2560; err = 0.35000000 * 2560; time = 0.0106s; samplesPerSecond = 242479.4
12/30/2017 01:13:58:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.18915710 * 2560; err = 0.36992188 * 2560; time = 0.0105s; samplesPerSecond = 242727.7
12/30/2017 01:13:58: Finished Epoch[ 1 of 2]: [Training] ce = 1.52387781 * 81920; err = 0.42613525 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.47537s
12/30/2017 01:13:58: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre2/cntkSpeech.1'

12/30/2017 01:13:58: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

12/30/2017 01:13:58: Starting minibatch loop.
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.18371334 * 2560; err = 0.35312500 * 2560; time = 0.0129s; samplesPerSecond = 198186.9
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.15387115 * 2560; err = 0.35195312 * 2560; time = 0.0118s; samplesPerSecond = 217790.8
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.15634022 * 2560; err = 0.35078125 * 2560; time = 0.0106s; samplesPerSecond = 242165.1
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.13972702 * 2560; err = 0.34531250 * 2560; time = 0.0105s; samplesPerSecond = 242969.5
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.14528008 * 2560; err = 0.36445312 * 2560; time = 0.0105s; samplesPerSecond = 242706.9
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.14011345 * 2560; err = 0.33476563 * 2560; time = 0.0105s; samplesPerSecond = 243649.4
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.09710693 * 2560; err = 0.33671875 * 2560; time = 0.0106s; samplesPerSecond = 242369.2
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.15687256 * 2560; err = 0.35546875 * 2560; time = 0.0105s; samplesPerSecond = 242771.4
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.16989441 * 2560; err = 0.35898438 * 2560; time = 0.0105s; samplesPerSecond = 243068.7
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.12214737 * 2560; err = 0.34804687 * 2560; time = 0.0106s; samplesPerSecond = 242488.5
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.12149048 * 2560; err = 0.34687500 * 2560; time = 0.0105s; samplesPerSecond = 243202.7
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.19465485 * 2560; err = 0.36406250 * 2560; time = 0.0105s; samplesPerSecond = 242819.7
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.13471985 * 2560; err = 0.34140625 * 2560; time = 0.0105s; samplesPerSecond = 243420.1
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.16473541 * 2560; err = 0.36054687 * 2560; time = 0.0105s; samplesPerSecond = 242997.2
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.12361755 * 2560; err = 0.35000000 * 2560; time = 0.0105s; samplesPerSecond = 243304.4
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09654999 * 2560; err = 0.34101562 * 2560; time = 0.0107s; samplesPerSecond = 240015.0
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.10468903 * 2560; err = 0.33945313 * 2560; time = 0.0105s; samplesPerSecond = 243018.0
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11432037 * 2560; err = 0.33007813 * 2560; time = 0.0105s; samplesPerSecond = 243427.0
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10250244 * 2560; err = 0.33906250 * 2560; time = 0.0105s; samplesPerSecond = 243276.6
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.06273804 * 2560; err = 0.32578125 * 2560; time = 0.0106s; samplesPerSecond = 241618.8
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.10996704 * 2560; err = 0.33750000 * 2560; time = 0.0119s; samplesPerSecond = 215126.1
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.13582306 * 2560; err = 0.35078125 * 2560; time = 0.0105s; samplesPerSecond = 242782.9
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12325439 * 2560; err = 0.34257813 * 2560; time = 0.0105s; samplesPerSecond = 243089.5
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.10882263 * 2560; err = 0.34023437 * 2560; time = 0.0105s; samplesPerSecond = 243376.1
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.09274902 * 2560; err = 0.33789063 * 2560; time = 0.0106s; samplesPerSecond = 240966.1
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.04688721 * 2560; err = 0.31914063 * 2560; time = 0.0105s; samplesPerSecond = 243126.5
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.04932556 * 2560; err = 0.33515625 * 2560; time = 0.0106s; samplesPerSecond = 242523.0
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.03842468 * 2560; err = 0.31992188 * 2560; time = 0.0106s; samplesPerSecond = 242318.7
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.06117859 * 2560; err = 0.33046875 * 2560; time = 0.0105s; samplesPerSecond = 242831.3
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.11318054 * 2560; err = 0.34492187 * 2560; time = 0.0105s; samplesPerSecond = 243082.6
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08679199 * 2560; err = 0.33515625 * 2560; time = 0.0109s; samplesPerSecond = 234318.5
12/30/2017 01:13:58:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04743347 * 2560; err = 0.32773438 * 2560; time = 0.0106s; samplesPerSecond = 240402.7
12/30/2017 01:13:58: Finished Epoch[ 2 of 2]: [Training] ce = 1.11559134 * 81920; err = 0.34248047 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.346705s
12/30/2017 01:13:58: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/Pre2/cntkSpeech'

12/30/2017 01:13:58: Action "train" complete.


12/30/2017 01:13:58: ##############################################################################
12/30/2017 01:13:58: #                                                                            #
12/30/2017 01:13:58: # addLayer3 command (edit action)                                            #
12/30/2017 01:13:58: #                                                                            #
12/30/2017 01:13:58: ##############################################################################


12/30/2017 01:13:58: Action "edit" complete.


12/30/2017 01:13:58: ##############################################################################
12/30/2017 01:13:58: #                                                                            #
12/30/2017 01:13:58: # speechTrain command (train action)                                         #
12/30/2017 01:13:58: #                                                                            #
12/30/2017 01:13:58: ##############################################################################

12/30/2017 01:13:58: 
Starting from checkpoint. Loading network from 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech.0'.
NDLBuilder Using GPU 0
reading script file D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/state.list
htkmlfreader: reading MLF file D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
12/30/2017 01:13:58: 
Model has 29 nodes. Using GPU 0.

12/30/2017 01:13:58: Training criterion:   ce = CrossEntropyWithSoftmax
12/30/2017 01:13:58: Evaluation criterion: err = ClassificationError

12/30/2017 01:13:58: Training 779396 parameters in 8 out of 8 parameter tensors and 20 nodes with gradient:

12/30/2017 01:13:58: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
12/30/2017 01:13:58: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
12/30/2017 01:13:58: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
12/30/2017 01:13:58: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
12/30/2017 01:13:58: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
12/30/2017 01:13:58: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
12/30/2017 01:13:58: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
12/30/2017 01:13:58: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

12/30/2017 01:13:58: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/30/2017 01:13:58: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

12/30/2017 01:13:58: Starting minibatch loop.
12/30/2017 01:13:58:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 4.09347076 * 2560; err = 0.82734375 * 2560; time = 0.0229s; samplesPerSecond = 111723.6
12/30/2017 01:13:58:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.57447586 * 2560; err = 0.64023438 * 2560; time = 0.0138s; samplesPerSecond = 184907.0
12/30/2017 01:13:58:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.03476105 * 2560; err = 0.54375000 * 2560; time = 0.0137s; samplesPerSecond = 187503.2
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.73747635 * 2560; err = 0.47617188 * 2560; time = 0.0137s; samplesPerSecond = 187495.0
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.55056915 * 2560; err = 0.43750000 * 2560; time = 0.0136s; samplesPerSecond = 187560.9
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.44497986 * 2560; err = 0.41054687 * 2560; time = 0.0138s; samplesPerSecond = 185791.3
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.36142731 * 2560; err = 0.40546875 * 2560; time = 0.0136s; samplesPerSecond = 187692.9
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35946808 * 2560; err = 0.39531250 * 2560; time = 0.0137s; samplesPerSecond = 187532.0
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.34237213 * 2560; err = 0.39570312 * 2560; time = 0.0137s; samplesPerSecond = 187244.0
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.30687866 * 2560; err = 0.37929687 * 2560; time = 0.0148s; samplesPerSecond = 173002.2
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.31363678 * 2560; err = 0.38750000 * 2560; time = 0.0137s; samplesPerSecond = 187446.9
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.24034119 * 2560; err = 0.37109375 * 2560; time = 0.0137s; samplesPerSecond = 187442.8
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.21518860 * 2560; err = 0.35937500 * 2560; time = 0.0137s; samplesPerSecond = 186824.5
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.24078827 * 2560; err = 0.36914063 * 2560; time = 0.0137s; samplesPerSecond = 186991.0
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.23589020 * 2560; err = 0.37265625 * 2560; time = 0.0136s; samplesPerSecond = 187961.6
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.20203857 * 2560; err = 0.35351563 * 2560; time = 0.0136s; samplesPerSecond = 187714.9
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.22040405 * 2560; err = 0.36328125 * 2560; time = 0.0140s; samplesPerSecond = 182242.7
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.25242310 * 2560; err = 0.38046875 * 2560; time = 0.0136s; samplesPerSecond = 187972.7
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.27484131 * 2560; err = 0.38554688 * 2560; time = 0.0139s; samplesPerSecond = 184527.1
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.22908630 * 2560; err = 0.38554688 * 2560; time = 0.0136s; samplesPerSecond = 187706.7
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.18030090 * 2560; err = 0.36093750 * 2560; time = 0.0136s; samplesPerSecond = 188022.4
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.19837036 * 2560; err = 0.36757812 * 2560; time = 0.0137s; samplesPerSecond = 187364.6
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.22071533 * 2560; err = 0.36406250 * 2560; time = 0.0136s; samplesPerSecond = 187934.0
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.19018860 * 2560; err = 0.35351563 * 2560; time = 0.0136s; samplesPerSecond = 188170.3
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.17300720 * 2560; err = 0.35742188 * 2560; time = 0.0136s; samplesPerSecond = 187906.5
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.12558594 * 2560; err = 0.35351563 * 2560; time = 0.0136s; samplesPerSecond = 187861.0
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.19316711 * 2560; err = 0.35898438 * 2560; time = 0.0137s; samplesPerSecond = 187471.6
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.13930359 * 2560; err = 0.35195312 * 2560; time = 0.0136s; samplesPerSecond = 188119.1
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.12437134 * 2560; err = 0.33515625 * 2560; time = 0.0136s; samplesPerSecond = 188153.7
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.10540161 * 2560; err = 0.34101562 * 2560; time = 0.0136s; samplesPerSecond = 188339.2
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.12473755 * 2560; err = 0.33515625 * 2560; time = 0.0137s; samplesPerSecond = 187418.1
12/30/2017 01:13:59:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.13026428 * 2560; err = 0.34765625 * 2560; time = 0.0136s; samplesPerSecond = 187631.0
12/30/2017 01:13:59: Finished Epoch[ 1 of 4]: [Training] ce = 1.41049786 * 81920; err = 0.40207520 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.577174s
12/30/2017 01:13:59: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech.1'

12/30/2017 01:13:59: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

12/30/2017 01:13:59: Starting minibatch loop.
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.21424761 * 5120; err = 0.36523438 * 5120; time = 0.0275s; samplesPerSecond = 185961.4
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.14997759 * 5120; err = 0.34570313 * 5120; time = 0.0186s; samplesPerSecond = 274996.8
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.10502491 * 5120; err = 0.33886719 * 5120; time = 0.0185s; samplesPerSecond = 276025.7
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.10657425 * 5120; err = 0.34101562 * 5120; time = 0.0185s; samplesPerSecond = 276346.0
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.16255798 * 5120; err = 0.36328125 * 5120; time = 0.0186s; samplesPerSecond = 275369.5
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.15796547 * 5120; err = 0.35898438 * 5120; time = 0.0186s; samplesPerSecond = 275409.5
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.13883362 * 5120; err = 0.34492187 * 5120; time = 0.0185s; samplesPerSecond = 276731.3
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.10509262 * 5120; err = 0.34648438 * 5120; time = 0.0185s; samplesPerSecond = 276022.7
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.11047897 * 5120; err = 0.34062500 * 5120; time = 0.0185s; samplesPerSecond = 276444.4
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.06133652 * 5120; err = 0.32734375 * 5120; time = 0.0185s; samplesPerSecond = 276193.9
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.10699844 * 5120; err = 0.33867188 * 5120; time = 0.0185s; samplesPerSecond = 276132.8
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.13975754 * 5120; err = 0.35292969 * 5120; time = 0.0185s; samplesPerSecond = 276478.8
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.04995117 * 5120; err = 0.32480469 * 5120; time = 0.0185s; samplesPerSecond = 276369.8
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02763824 * 5120; err = 0.32285156 * 5120; time = 0.0186s; samplesPerSecond = 275955.7
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.08706665 * 5120; err = 0.32988281 * 5120; time = 0.0185s; samplesPerSecond = 276098.6
12/30/2017 01:13:59:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06690369 * 5120; err = 0.32167969 * 5120; time = 0.0190s; samplesPerSecond = 270042.2
12/30/2017 01:13:59: Finished Epoch[ 2 of 4]: [Training] ce = 1.11190033 * 81920; err = 0.34145508 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.309976s
12/30/2017 01:13:59: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech.2'

12/30/2017 01:13:59: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

12/30/2017 01:13:59: Starting minibatch loop.
12/30/2017 01:13:59:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.12241631 * 5120; err = 0.34179688 * 5120; time = 0.0199s; samplesPerSecond = 256838.6
12/30/2017 01:13:59:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.08498554 * 5120; err = 0.33613281 * 5120; time = 0.0188s; samplesPerSecond = 272169.6
12/30/2017 01:13:59:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.09316616 * 5120; err = 0.33828125 * 5120; time = 0.0186s; samplesPerSecond = 274766.6
12/30/2017 01:13:59:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.10422935 * 5120; err = 0.33828125 * 5120; time = 0.0187s; samplesPerSecond = 274070.9
12/30/2017 01:13:59:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.07953606 * 5120; err = 0.33261719 * 5120; time = 0.0186s; samplesPerSecond = 275502.8
12/30/2017 01:13:59:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.05399208 * 5120; err = 0.33085938 * 5120; time = 0.0186s; samplesPerSecond = 275698.7
12/30/2017 01:13:59:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.06956406 * 5120; err = 0.32539062 * 5120; time = 0.0191s; samplesPerSecond = 267747.4
12/30/2017 01:14:00:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.07985764 * 5120; err = 0.32792969 * 5120; time = 0.0188s; samplesPerSecond = 272810.6
12/30/2017 01:14:00:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.05197220 * 5120; err = 0.32617188 * 5120; time = 0.0186s; samplesPerSecond = 274818.2
12/30/2017 01:14:00:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.05977478 * 5120; err = 0.33046875 * 5120; time = 0.0186s; samplesPerSecond = 274822.6
12/30/2017 01:14:00:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.04717255 * 5120; err = 0.32792969 * 5120; time = 0.0188s; samplesPerSecond = 272896.4
12/30/2017 01:14:00:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.08563919 * 5120; err = 0.33964844 * 5120; time = 0.0190s; samplesPerSecond = 269109.7
12/30/2017 01:14:00:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.10514679 * 5120; err = 0.32910156 * 5120; time = 0.0186s; samplesPerSecond = 275348.8
12/30/2017 01:14:00:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.06171722 * 5120; err = 0.32109375 * 5120; time = 0.0186s; samplesPerSecond = 274619.2
12/30/2017 01:14:00:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.05704346 * 5120; err = 0.33281250 * 5120; time = 0.0192s; samplesPerSecond = 267275.0
12/30/2017 01:14:00:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.04391479 * 5120; err = 0.32871094 * 5120; time = 0.0261s; samplesPerSecond = 196270.9
12/30/2017 01:14:00: Finished Epoch[ 3 of 4]: [Training] ce = 1.07500801 * 81920; err = 0.33170166 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.311769s
12/30/2017 01:14:00: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech.3'

12/30/2017 01:14:00: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

12/30/2017 01:14:00: Starting minibatch loop.
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.03005266 * 5120; err = 0.32695313 * 5120; time = 0.0200s; samplesPerSecond = 256528.4
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.04017516 * 4926; err = 0.31303289 * 4926; time = 0.0616s; samplesPerSecond = 80002.5
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.02174778 * 5120; err = 0.32285156 * 5120; time = 0.0186s; samplesPerSecond = 275871.0
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.01952095 * 5120; err = 0.31582031 * 5120; time = 0.0185s; samplesPerSecond = 276220.7
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.00513496 * 5120; err = 0.31972656 * 5120; time = 0.0185s; samplesPerSecond = 276147.7
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 0.99812813 * 5120; err = 0.31367187 * 5120; time = 0.0185s; samplesPerSecond = 276104.6
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.99019432 * 5120; err = 0.30546875 * 5120; time = 0.0185s; samplesPerSecond = 276097.1
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.01111908 * 5120; err = 0.31445313 * 5120; time = 0.0186s; samplesPerSecond = 275528.1
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99833145 * 5120; err = 0.31289062 * 5120; time = 0.0186s; samplesPerSecond = 275661.6
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.96827850 * 5120; err = 0.31464844 * 5120; time = 0.0185s; samplesPerSecond = 276414.6
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 0.99143982 * 5120; err = 0.30781250 * 5120; time = 0.0186s; samplesPerSecond = 275505.8
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.00068130 * 5120; err = 0.31035156 * 5120; time = 0.0186s; samplesPerSecond = 275719.5
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.00592957 * 5120; err = 0.31328125 * 5120; time = 0.0187s; samplesPerSecond = 274452.8
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.98450165 * 5120; err = 0.30976562 * 5120; time = 0.0187s; samplesPerSecond = 274514.6
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.93657532 * 5120; err = 0.29726562 * 5120; time = 0.0186s; samplesPerSecond = 275581.4
12/30/2017 01:14:00:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.96798401 * 5120; err = 0.30292969 * 5120; time = 0.0186s; samplesPerSecond = 275722.4
12/30/2017 01:14:00: Finished Epoch[ 4 of 4]: [Training] ce = 0.99785318 * 81920; err = 0.31252441 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.346013s
12/30/2017 01:14:00: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech'

12/30/2017 01:14:00: Action "train" complete.


12/30/2017 01:14:00: ##############################################################################
12/30/2017 01:14:00: #                                                                            #
12/30/2017 01:14:00: # sequenceTrain command (train action)                                       #
12/30/2017 01:14:00: #                                                                            #
12/30/2017 01:14:00: ##############################################################################

12/30/2017 01:14:00: 
Creating virgin network.
Load: Loading model file: D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech
Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *8]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *8]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *8], [363 x 1], [363 x 1] -> [363 x *8]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *8] -> [512 x *8]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *8], [512 x 1] -> [512 x 1 x *8]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *8], [512 x 1] -> [512 x 1 x *8]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *8], [512 x 1] -> [512 x 1 x *8]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *8] -> [132 x 1 x *8]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *8], [132 x 1] -> [132 x 1 x *8]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *8], [132 x 1 x *8] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *8], [132 x 1 x *8] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *8], [132 x 1] -> [132 x 1 x *8]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.




Post-processing network complete.

CloneFunction: (features : InputValue) -> [
    netEval = OL.z : Plus
    scaledLogLikelihood = scaledLogLikelihood : Minus
]
clonedmodel.HL2.t.inputs[0] = HL2.W (162) ==>  clonedmodel.HL2.W (181)
clonedmodel.HL2.t.inputs[1] = HL1.y (158) ==>  clonedmodel.HL1.y (198)
clonedmodel.HL3.t.inputs[0] = HL3.W (167) ==>  clonedmodel.HL3.W (183)
clonedmodel.HL3.t.inputs[1] = HL2.y (163) ==>  clonedmodel.HL2.y (199)
clonedmodel.OL.t.inputs[0] = OL.W (174) ==>  clonedmodel.OL.W (191)
clonedmodel.OL.t.inputs[1] = HL3.y (168) ==>  clonedmodel.HL3.y (201)
clonedmodel.HL1.t.inputs[0] = HL1.W (157) ==>  clonedmodel.HL1.W (185)
clonedmodel.HL1.t.inputs[1] = featNorm (150) ==>  clonedmodel.featNorm (196)
clonedmodel.featNorm.inputs[0] = features (151) ==>  features (180)
clonedmodel.featNorm.inputs[1] = globalMean (153) ==>  clonedmodel.globalMean (193)
clonedmodel.featNorm.inputs[2] = globalInvStd (152) ==>  clonedmodel.globalInvStd (194)
clonedmodel.HL1.z.inputs[0] = HL1.t (156) ==>  clonedmodel.HL1.t (190)
clonedmodel.HL1.z.inputs[1] = HL1.b (155) ==>  clonedmodel.HL1.b (184)
clonedmodel.HL1.y.inputs[0] = HL1.z (159) ==>  clonedmodel.HL1.z (197)
clonedmodel.HL2.y.inputs[0] = HL2.z (164) ==>  clonedmodel.HL2.z (200)
clonedmodel.HL2.z.inputs[0] = HL2.t (161) ==>  clonedmodel.HL2.t (186)
clonedmodel.HL2.z.inputs[1] = HL2.b (160) ==>  clonedmodel.HL2.b (192)
clonedmodel.HL3.y.inputs[0] = HL3.z (169) ==>  clonedmodel.HL3.z (202)
clonedmodel.HL3.z.inputs[0] = HL3.t (166) ==>  clonedmodel.HL3.t (187)
clonedmodel.HL3.z.inputs[1] = HL3.b (165) ==>  clonedmodel.HL3.b (182)
clonedmodel.OL.z.inputs[0] = OL.t (173) ==>  clonedmodel.OL.t (189)
clonedmodel.OL.z.inputs[1] = OL.b (172) ==>  clonedmodel.OL.b (188)
clonedmodel.scaledLogLikelihood.inputs[0] = OL.z (175) ==>  clonedmodel.OL.z (203)
clonedmodel.scaledLogLikelihood.inputs[1] = logPrior (171) ==>  clonedmodel.logPrior (205)
clonedmodel.logPrior.inputs[0] = globalPrior (154) ==>  clonedmodel.globalPrior (195)
CloneFunction: Cloned 25 nodes and relinked 25 inputs.
12/30/2017 01:14:00: Reading files
 D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/CY2SCH010061231_1369712653.numden.lats.symlist 
 D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/model.overalltying 
 D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/state.list 
 D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/model.transprob 
simplesenonehmm: reading 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/model.overalltying', 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/state.list', 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read

Post-processing network...

4 roots:
	Err = ClassificationError()
	clonedmodel.scaledLogLikelihood = Minus()
	cr = SequenceWithLattice()
	latticeAxis = DynamicAxis()

Validating network. 31 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> clonedmodel.OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> clonedmodel.HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> clonedmodel.HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> clonedmodel.HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> clonedmodel.globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> clonedmodel.globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> clonedmodel.featNorm = PerDimMeanVarNormalization (features, clonedmodel.globalMean, clonedmodel.globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> clonedmodel.HL1.t = Times (clonedmodel.HL1.W, clonedmodel.featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> clonedmodel.HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> clonedmodel.HL1.z = Plus (clonedmodel.HL1.t, clonedmodel.HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> clonedmodel.HL1.y = Sigmoid (clonedmodel.HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.HL2.t = Times (clonedmodel.HL2.W, clonedmodel.HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> clonedmodel.HL2.z = Plus (clonedmodel.HL2.t, clonedmodel.HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> clonedmodel.HL2.y = Sigmoid (clonedmodel.HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.HL3.t = Times (clonedmodel.HL3.W, clonedmodel.HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> clonedmodel.HL3.z = Plus (clonedmodel.HL3.t, clonedmodel.HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> clonedmodel.HL3.y = Sigmoid (clonedmodel.HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.OL.t = Times (clonedmodel.OL.W, clonedmodel.HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> clonedmodel.OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> clonedmodel.OL.z = Plus (clonedmodel.OL.t, clonedmodel.OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> Err = ClassificationError (labels, clonedmodel.OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> clonedmodel.globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> clonedmodel.logPrior = Log (clonedmodel.globalPrior) : [132 x 1] -> [132 x 1]
Validating --> clonedmodel.scaledLogLikelihood = Minus (clonedmodel.OL.z, clonedmodel.logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> lattice = InputValue() :  -> [1 x latticeAxis]
Validating --> cr = SequenceWithLattice (labels, clonedmodel.OL.z, clonedmodel.scaledLogLikelihood, lattice) : [132 x *7], [132 x 1 x *7], [132 x 1 x *7], [1 x latticeAxis] -> [1]
Validating --> latticeAxis = DynamicAxis() :  -> [1 x 1 x latticeAxis]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.




Post-processing network complete.

Reading script file D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/glob_0000.scp ... 948 entries
HTKDeserializer: selected '948' utterances grouped into '3' chunks, average chunk size: 316.0 utterances, 84244.7 frames (for I/O: 316.0 utterances, 84244.7 frames)
HTKDeserializer: determined feature kind as '33'-dimensional 'USER' with frame shift 10.0 ms
Total (133) state names in state list 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/state.list'
MLFDeserializer: '948' utterances with '252734' frames
Reading lattice index file D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu\TestData/latticeIndex.txt ...LatticeDeserializer: '923' sequences
12/30/2017 01:14:00: 
Model has 31 nodes. Using GPU 0.

12/30/2017 01:14:00: Training criterion:   cr = SequenceWithLattice
12/30/2017 01:14:00: Evaluation criterion: Err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 6 are aliased.
	clonedmodel.HL2.t (gradient) reuses clonedmodel.HL2.z (gradient)
	clonedmodel.OL.t (gradient) reuses clonedmodel.OL.z (gradient)
	clonedmodel.HL3.t (gradient) reuses clonedmodel.HL3.z (gradient)

Memory Sharing: Out of 52 matrices, 29 are shared as 7, and 23 are not shared.

Here are the ones that share memory:
	{ clonedmodel.HL3.W : [512 x 512] (gradient)
	  clonedmodel.OL.t : [132 x 1 x *7] (gradient)
	  clonedmodel.OL.z : [132 x 1 x *7] (gradient) }
	{ clonedmodel.HL1.z : [512 x 1 x *7]
	  clonedmodel.HL2.W : [512 x 512] (gradient)
	  clonedmodel.HL2.t : [512 x 1 x *7]
	  clonedmodel.HL2.y : [512 x 1 x *7] }
	{ clonedmodel.HL2.b : [512 x 1] (gradient)
	  clonedmodel.HL2.y : [512 x 1 x *7] (gradient)
	  clonedmodel.HL2.z : [512 x 1 x *7]
	  clonedmodel.HL3.t : [512 x 1 x *7]
	  clonedmodel.HL3.y : [512 x 1 x *7] }
	{ clonedmodel.HL1.t : [512 x *7] (gradient)
	  clonedmodel.HL1.y : [512 x 1 x *7] (gradient)
	  clonedmodel.HL3.y : [512 x 1 x *7] (gradient)
	  clonedmodel.OL.t : [132 x 1 x *7]
	  clonedmodel.scaledLogLikelihood : [132 x 1 x *7] }
	{ clonedmodel.OL.W : [132 x 512] (gradient)
	  clonedmodel.scaledLogLikelihood : [132 x 1 x *7] (gradient) }
	{ clonedmodel.HL1.W : [512 x 363] (gradient)
	  clonedmodel.HL1.t : [512 x *7]
	  clonedmodel.HL1.y : [512 x 1 x *7] }
	{ clonedmodel.HL1.z : [512 x 1 x *7] (gradient)
	  clonedmodel.HL2.t : [512 x 1 x *7] (gradient)
	  clonedmodel.HL2.z : [512 x 1 x *7] (gradient)
	  clonedmodel.HL3.t : [512 x 1 x *7] (gradient)
	  clonedmodel.HL3.z : [512 x 1 x *7]
	  clonedmodel.HL3.z : [512 x 1 x *7] (gradient)
	  clonedmodel.OL.z : [132 x 1 x *7] }

Here are the ones that don't share memory:
	{latticeAxis : [1 x 1 x latticeAxis]}
	{clonedmodel.logPrior : [132 x 1]}
	{Err : [1]}
	{cr : [1]}
	{clonedmodel.HL2.W : [512 x 512]}
	{labels : [132 x *7]}
	{features : [363 x *7]}
	{clonedmodel.OL.b : [132 x 1] (gradient)}
	{clonedmodel.featNorm : [363 x *7]}
	{clonedmodel.HL3.b : [512 x 1] (gradient)}
	{clonedmodel.HL1.b : [512 x 1] (gradient)}
	{cr : [1] (gradient)}
	{clonedmodel.OL.W : [132 x 512]}
	{clonedmodel.HL1.b : [512 x 1]}
	{clonedmodel.OL.b : [132 x 1]}
	{clonedmodel.HL2.b : [512 x 1]}
	{clonedmodel.HL3.b : [512 x 1]}
	{clonedmodel.HL1.W : [512 x 363]}
	{clonedmodel.HL3.W : [512 x 512]}
	{clonedmodel.globalInvStd : [363 x 1]}
	{lattice : [1 x latticeAxis]}
	{clonedmodel.globalPrior : [132 x 1]}
	{clonedmodel.globalMean : [363 x 1]}


12/30/2017 01:14:00: Training 779396 parameters in 8 out of 8 parameter tensors and 21 nodes with gradient:

12/30/2017 01:14:00: 	Node 'clonedmodel.HL1.W' (LearnableParameter operation) : [512 x 363]
12/30/2017 01:14:00: 	Node 'clonedmodel.HL1.b' (LearnableParameter operation) : [512 x 1]
12/30/2017 01:14:00: 	Node 'clonedmodel.HL2.W' (LearnableParameter operation) : [512 x 512]
12/30/2017 01:14:00: 	Node 'clonedmodel.HL2.b' (LearnableParameter operation) : [512 x 1]
12/30/2017 01:14:00: 	Node 'clonedmodel.HL3.W' (LearnableParameter operation) : [512 x 512]
12/30/2017 01:14:00: 	Node 'clonedmodel.HL3.b' (LearnableParameter operation) : [512 x 1]
12/30/2017 01:14:00: 	Node 'clonedmodel.OL.W' (LearnableParameter operation) : [132 x 512]
12/30/2017 01:14:00: 	Node 'clonedmodel.OL.b' (LearnableParameter operation) : [132 x 1]

12/30/2017 01:14:00: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/30/2017 01:14:01: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.000000  momentum as time constant = 2432.7 samples

12/30/2017 01:14:01: Starting minibatch loop.
dengamma value 1.081426
dengamma value 1.045253
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.070617
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.063890
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.123827
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.056923
dengamma value 1.033068
parallelforwardbackwardlattice: 109 launches for forward, 109 launches for backward
dengamma value 0.975058
dengamma value 1.100492
dengamma value 1.122266
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.087244
dengamma value 0.994876
dengamma value 1.053050
parallelforwardbackwardlattice: 35 launches for forward, 35 launches for backward
dengamma value 1.007732
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.112844
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.105688
parallelforwardbackwardlattice: 114 launches for forward, 114 launches for backward
dengamma value 1.144841
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 1.035460
dengamma value 1.094303
dengamma value 1.166204
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.110020
parallelforwardbackwardlattice: 95 launches for forward, 95 launches for backward
dengamma value 1.101345
dengamma value 1.070974
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.112870
parallelforwardbackwardlattice: 12 launches for forward, 12 launches for backward
dengamma value 1.165066
parallelforwardbackwardlattice: 70 launches for forward, 70 launches for backward
dengamma value 1.138096
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.032124
dengamma value 1.003294
parallelforwardbackwardlattice: 78 launches for forward, 78 launches for backward
dengamma value 1.055434
parallelforwardbackwardlattice: 63 launches for forward, 63 launches for backward
dengamma value 1.028479
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.018273
12/30/2017 01:14:02: Finished Epoch[ 1 of 3]: [Training] cr = 0.07994324 * 9518; Err = 0.31319605 * 9518; totalSamplesSeen = 9518; learningRatePerSample = 2e-06; epochTime=1.73508s
12/30/2017 01:14:02: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech.sequence.1'

12/30/2017 01:14:02: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.000000  momentum as time constant = 2432.7 samples

12/30/2017 01:14:02: Starting minibatch loop.
parallelforwardbackwardlattice: 71 launches for forward, 71 launches for backward
dengamma value 1.059391
dengamma value 1.107176
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.081665
WARNING: The same matrix with dim [1, 1224] has been transferred between different devices for 20 times.
dengamma value 1.108651
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.033408
parallelforwardbackwardlattice: 62 launches for forward, 62 launches for backward
dengamma value 1.055023
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 1.037726
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.099203
parallelforwardbackwardlattice: 84 launches for forward, 84 launches for backward
dengamma value 1.097348
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.295740
parallelforwardbackwardlattice: 119 launches for forward, 119 launches for backward
dengamma value 1.110673
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.124087
parallelforwardbackwardlattice: 52 launches for forward, 52 launches for backward
dengamma value 1.105843
dengamma value 1.049773
dengamma value 1.019302
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.080679
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.094493
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.095618
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.114931
dengamma value 1.077371
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.054599
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.303497
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.075010
dengamma value 1.013192
dengamma value 1.073627
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.082927
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.088678
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.008281
parallelforwardbackwardlattice: 146 launches for forward, 146 launches for backward
dengamma value 1.177165
dengamma value 1.025230
parallelforwardbackwardlattice: 31 launches for forward, 31 launches for backward
dengamma value 1.063327
dengamma value 1.074971
dengamma value 1.025291
parallelforwardbackwardlattice: 46 launches for forward, 46 launches for backward
dengamma value 1.094358
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.110812
dengamma value 1.063590
12/30/2017 01:14:03: Finished Epoch[ 2 of 3]: [Training] cr = 0.07822417 * 10188; Err = 0.29897919 * 10188; totalSamplesSeen = 19706; learningRatePerSample = 2e-06; epochTime=1.16421s
12/30/2017 01:14:04: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech.sequence.2'

12/30/2017 01:14:04: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.000000  momentum as time constant = 2432.7 samples

12/30/2017 01:14:04: Starting minibatch loop.
dengamma value 1.040831
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.134609
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.080426
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.092333
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.096313
parallelforwardbackwardlattice: 83 launches for forward, 83 launches for backward
dengamma value 1.057851
dengamma value 1.084177
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.026227
parallelforwardbackwardlattice: 19 launches for forward, 19 launches for backward
dengamma value 1.108293
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.062587
dengamma value 1.117344
dengamma value 1.068591
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.173734
parallelforwardbackwardlattice: 123 launches for forward, 123 launches for backward
dengamma value 1.141242
parallelforwardbackwardlattice: 104 launches for forward, 104 launches for backward
dengamma value 1.111870
dengamma value 1.090825
parallelforwardbackwardlattice: 12 launches for forward, 12 launches for backward
dengamma value 1.038615
dengamma value 1.041119
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.073870
parallelforwardbackwardlattice: 68 launches for forward, 68 launches for backward
dengamma value 1.076027
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.149449
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.040236
parallelforwardbackwardlattice: 105 launches for forward, 105 launches for backward
dengamma value 1.055438
parallelforwardbackwardlattice: 18 launches for forward, 18 launches for backward
dengamma value 1.036899
parallelforwardbackwardlattice: 86 launches for forward, 86 launches for backward
dengamma value 1.135858
dengamma value 1.043859
dengamma value 0.975584
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 1.017269
dengamma value 1.012629
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 1.091356
dengamma value 1.202701
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.132253
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 1.046401
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.058900
dengamma value 1.108589
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.107581
12/30/2017 01:14:05: Finished Epoch[ 3 of 3]: [Training] cr = 0.07634912 * 9878; Err = 0.32779915 * 9878; totalSamplesSeen = 29584; learningRatePerSample = 2e-06; epochTime=1.14933s
12/30/2017 01:14:05: SGD: Saving checkpoint model 'D:\cntk-test-20171230011302.7227\Speech\DNN_SequenceTrainingNewReader@release_gpu/models/cntkSpeech.sequence'

12/30/2017 01:14:05: Action "train" complete.

12/30/2017 01:14:05: __COMPLETED__