CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz
    Hardware threads: 12
    Total Memory: 57700428 kB
-------------------------------------------------------------------
=== Running /home/ubuntu/workspace/build/gpu/debug/bin/cntk configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/cntk_sequence.cntk currentDirectory=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData RunDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu DataDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader OutputDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu DeviceId=0 timestamping=true
CNTK 2.3.1+ (HEAD eff656, Dec 31 2017 19:25:49) at 2017/12/31 19:37:02

/home/ubuntu/workspace/build/gpu/debug/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/cntk_sequence.cntk  currentDirectory=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData  RunDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu  DataDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader  OutputDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu  DeviceId=0  timestamping=true
Changed current directory to /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData
12/31/2017 19:37:02: -------------------------------------------------------------------
12/31/2017 19:37:02: Build info: 

12/31/2017 19:37:02: 		Built time: Dec 31 2017 19:23:52
12/31/2017 19:37:02: 		Last modified date: Sun Dec 31 07:28:49 2017
12/31/2017 19:37:02: 		Build type: debug
12/31/2017 19:37:02: 		Build target: GPU
12/31/2017 19:37:02: 		With 1bit-SGD: no
12/31/2017 19:37:02: 		With ASGD: yes
12/31/2017 19:37:02: 		Math lib: mkl
12/31/2017 19:37:02: 		CUDA version: 9.0.0
12/31/2017 19:37:02: 		CUDNN version: 6.0.21
12/31/2017 19:37:02: 		Build Branch: HEAD
12/31/2017 19:37:02: 		Build SHA1: eff6566844242a71f588b229f65ab56555827369
12/31/2017 19:37:02: 		MPI distribution: Open MPI
12/31/2017 19:37:02: 		MPI version: 1.10.7
12/31/2017 19:37:02: -------------------------------------------------------------------
12/31/2017 19:37:02: -------------------------------------------------------------------
12/31/2017 19:37:02: GPU info:

12/31/2017 19:37:02: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 8112 MB
12/31/2017 19:37:02: -------------------------------------------------------------------

Configuration, Raw:

12/31/2017 19:37:02: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:sequenceTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
sequenceTrain = [
    action = "train"
    modelPath = $RunDir$/models/cntkSpeech.sequence
    traceLevel = 1
    SGD = [
        epochSize = 6000000
        minibatchSize = 800000
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
		maxEpochs = 3
		gradientClippingWithTruncation = true
		clippingThresholdPerSample = 1.0
    ]
	reader = [
			verbosity = 0
			randomize = false
			deserializers = (
				[
					type = "HTKFeatureDeserializer"
					module = "HTKDeserializers"
					input = [
						features = [
							dim=363
							scpFile = "$DataDir$/glob_0000.scp"
						]
					]
				]:
				[
					type = "HTKMLFDeserializer"
					module = "HTKDeserializers"
					input = [
						labels = [
							dim = 132
							mlfFile="$DataDir$/glob_0000.mlf"
							labelMappingFile = "$DataDir$/state.list" 
						]
					]
				]:
				[
					type = "LatticeDeserializer"
					module = "HTKDeserializers"
					input = [
						lattice=[
							latticeIndexFile="$DataDir$/latticeIndex.txt"
						]
					]
				]
			)
		]
		BrainScriptNetworkBuilder = {
			baseFeatDim = 33
			featDim = 11 * baseFeatDim
			labelDim = 132
			latticeAxis = DynamicAxis()
			features = Input{featDim}
			labels = Input{labelDim}
			lattice = Input{1,dynamicAxis=latticeAxis}
			featExtNetwork  = BS.Network.Load("$RunDir$/models/cntkSpeech")
			featExt = BS.Network.CloneFunction (
              (featExtNetwork.features),
              [netEval = featExtNetwork.OL_z;scaledLogLikelihood = featExtNetwork.scaledLogLikelihood ],
              parameters="learnable")
			clonedmodel= featExt(features)
			cr = SequenceWithLattice(labels, clonedmodel.netEval, clonedmodel.scaledLogLikelihood, lattice, "$DataDir$/CY2SCH010061231_1369712653.numden.lats.symlist", "$DataDir$/model.overalltying", "$DataDir$/state.list", "$DataDir$/model.transprob", tag="criterion")  
			Err = ClassificationError(labels,clonedmodel.netEval,tag="evaluation");
		}
]
currentDirectory=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData
RunDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu
DataDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData
ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader
OutputDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu
DeviceId=0
timestamping=true


Configuration After Variable Resolution:

12/31/2017 19:37:02: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:sequenceTrain
ndlMacros = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.0"
    editPath  = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.mlf"
        labelMappingFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
sequenceTrain = [
    action = "train"
    modelPath = /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.sequence
    traceLevel = 1
    SGD = [
        epochSize = 6000000
        minibatchSize = 800000
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
		maxEpochs = 3
		gradientClippingWithTruncation = true
		clippingThresholdPerSample = 1.0
    ]
	reader = [
			verbosity = 0
			randomize = false
			deserializers = (
				[
					type = "HTKFeatureDeserializer"
					module = "HTKDeserializers"
					input = [
						features = [
							dim=363
							scpFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.scp"
						]
					]
				]:
				[
					type = "HTKMLFDeserializer"
					module = "HTKDeserializers"
					input = [
						labels = [
							dim = 132
							mlfFile="/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.mlf"
							labelMappingFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list" 
						]
					]
				]:
				[
					type = "LatticeDeserializer"
					module = "HTKDeserializers"
					input = [
						lattice=[
							latticeIndexFile="/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/latticeIndex.txt"
						]
					]
				]
			)
		]
		BrainScriptNetworkBuilder = {
			baseFeatDim = 33
			featDim = 11 * baseFeatDim
			labelDim = 132
			latticeAxis = DynamicAxis()
			features = Input{featDim}
			labels = Input{labelDim}
			lattice = Input{1,dynamicAxis=latticeAxis}
			featExtNetwork  = BS.Network.Load("/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech")
			featExt = BS.Network.CloneFunction (
              (featExtNetwork.features),
              [netEval = featExtNetwork.OL_z;scaledLogLikelihood = featExtNetwork.scaledLogLikelihood ],
              parameters="learnable")
			clonedmodel= featExt(features)
			cr = SequenceWithLattice(labels, clonedmodel.netEval, clonedmodel.scaledLogLikelihood, lattice, "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/CY2SCH010061231_1369712653.numden.lats.symlist", "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/model.overalltying", "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list", "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/model.transprob", tag="criterion")  
			Err = ClassificationError(labels,clonedmodel.netEval,tag="evaluation");
		}
]
currentDirectory=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData
RunDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu
DataDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData
ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader
OutputDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu
DeviceId=0
timestamping=true


Configuration After Processing and Variable Resolution:

configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.0"
    editPath  = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader
configparameters: cntk_sequence.cntk:currentDirectory=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData
configparameters: cntk_sequence.cntk:DataDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.mlf"
        labelMappingFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:RunDir=/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.sequence
    traceLevel = 1
    SGD = [
        epochSize = 6000000
        minibatchSize = 800000
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
		maxEpochs = 3
		gradientClippingWithTruncation = true
		clippingThresholdPerSample = 1.0
    ]
	reader = [
			verbosity = 0
			randomize = false
			deserializers = (
				[
					type = "HTKFeatureDeserializer"
					module = "HTKDeserializers"
					input = [
						features = [
							dim=363
							scpFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.scp"
						]
					]
				]:
				[
					type = "HTKMLFDeserializer"
					module = "HTKDeserializers"
					input = [
						labels = [
							dim = 132
							mlfFile="/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.mlf"
							labelMappingFile = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list" 
						]
					]
				]:
				[
					type = "LatticeDeserializer"
					module = "HTKDeserializers"
					input = [
						lattice=[
							latticeIndexFile="/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/latticeIndex.txt"
						]
					]
				]
			)
		]
		BrainScriptNetworkBuilder = {
			baseFeatDim = 33
			featDim = 11 * baseFeatDim
			labelDim = 132
			latticeAxis = DynamicAxis()
			features = Input{featDim}
			labels = Input{labelDim}
			lattice = Input{1,dynamicAxis=latticeAxis}
			featExtNetwork  = BS.Network.Load("/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech")
			featExt = BS.Network.CloneFunction (
              (featExtNetwork.features),
              [netEval = featExtNetwork.OL_z;scaledLogLikelihood = featExtNetwork.scaledLogLikelihood ],
              parameters="learnable")
			clonedmodel= featExt(features)
			cr = SequenceWithLattice(labels, clonedmodel.netEval, clonedmodel.scaledLogLikelihood, lattice, "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/CY2SCH010061231_1369712653.numden.lats.symlist", "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/model.overalltying", "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list", "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/model.transprob", tag="criterion")  
			Err = ClassificationError(labels,clonedmodel.netEval,tag="evaluation");
		}
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/ubuntu/workspace/Tests/EndToEndTests/Speech/DNN/SequenceTrainingNewReader/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
12/31/2017 19:37:02: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain sequenceTrain
12/31/2017 19:37:02: precision = "float"

12/31/2017 19:37:02: ##############################################################################
12/31/2017 19:37:02: #                                                                            #
12/31/2017 19:37:02: # dptPre1 command (train action)                                             #
12/31/2017 19:37:02: #                                                                            #
12/31/2017 19:37:02: ##############################################################################

12/31/2017 19:37:02: 
Creating virgin network.
NDLBuilder Using GPU 0
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
reading script file /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
12/31/2017 19:37:02: 
Model has 19 nodes. Using GPU 0.

12/31/2017 19:37:02: Training criterion:   ce = CrossEntropyWithSoftmax
12/31/2017 19:37:02: Evaluation criterion: err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 2 are aliased.
	OL.t (gradient) reuses OL.z (gradient)

Memory Sharing: Out of 29 matrices, 12 are shared as 3, and 17 are not shared.

Here are the ones that share memory:
	{ HL1.t : [512 x *]
	  HL1.t : [512 x *] (gradient)
	  HL1.y : [512 x 1 x *] (gradient)
	  OL.z : [132 x 1 x *] }
	{ HL1.W : [512 x 363] (gradient)
	  HL1.z : [512 x 1 x *]
	  HL1.z : [512 x 1 x *] (gradient)
	  OL.t : [132 x 1 x *]
	  OL.t : [132 x 1 x *] (gradient)
	  OL.z : [132 x 1 x *] (gradient) }
	{ HL1.b : [512 x 1] (gradient)
	  HL1.y : [512 x 1 x *] }

Here are the ones that don't share memory:
	{scaledLogLikelihood : [132 x 1 x *]}
	{OL.b : [132 x 1] (gradient)}
	{ce : [1] (gradient)}
	{featNorm : [363 x *]}
	{err : [1]}
	{logPrior : [132 x 1]}
	{globalMean : [363 x 1]}
	{OL.W : [132 x 512] (gradient)}
	{ce : [1]}
	{globalPrior : [132 x 1]}
	{globalInvStd : [363 x 1]}
	{HL1.W : [512 x 363]}
	{HL1.b : [512 x 1]}
	{OL.W : [132 x 512]}
	{OL.b : [132 x 1]}
	{labels : [132 x *]}
	{features : [363 x *]}


12/31/2017 19:37:02: Training 254084 parameters in 4 out of 4 parameter tensors and 10 nodes with gradient:

12/31/2017 19:37:02: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
12/31/2017 19:37:02: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
12/31/2017 19:37:02: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
12/31/2017 19:37:02: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

12/31/2017 19:37:02: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/31/2017 19:37:02: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

12/31/2017 19:37:03: Starting minibatch loop.
12/31/2017 19:37:03:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.74183807 * 2560; err = 0.80195313 * 2560; time = 0.2298s; samplesPerSecond = 11138.2
12/31/2017 19:37:03:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.91124802 * 2560; err = 0.70898438 * 2560; time = 0.0500s; samplesPerSecond = 51233.0
12/31/2017 19:37:03:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.58016052 * 2560; err = 0.66640625 * 2560; time = 0.0501s; samplesPerSecond = 51067.7
12/31/2017 19:37:03:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.27427139 * 2560; err = 0.58750000 * 2560; time = 0.0506s; samplesPerSecond = 50573.9
12/31/2017 19:37:03:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 2.05503540 * 2560; err = 0.56093750 * 2560; time = 0.0518s; samplesPerSecond = 49407.7
12/31/2017 19:37:03:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.91055145 * 2560; err = 0.52812500 * 2560; time = 0.0496s; samplesPerSecond = 51570.7
12/31/2017 19:37:03:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.81562805 * 2560; err = 0.51171875 * 2560; time = 0.0505s; samplesPerSecond = 50712.4
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.68803253 * 2560; err = 0.48476562 * 2560; time = 0.0501s; samplesPerSecond = 51063.3
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.57382050 * 2560; err = 0.45429687 * 2560; time = 0.0527s; samplesPerSecond = 48579.0
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62090302 * 2560; err = 0.47304687 * 2560; time = 0.0502s; samplesPerSecond = 51026.0
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.59272614 * 2560; err = 0.47500000 * 2560; time = 0.0512s; samplesPerSecond = 50027.7
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.51520386 * 2560; err = 0.44531250 * 2560; time = 0.0497s; samplesPerSecond = 51461.0
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.49181824 * 2560; err = 0.45039062 * 2560; time = 0.0501s; samplesPerSecond = 51144.5
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.53703613 * 2560; err = 0.44804688 * 2560; time = 0.0501s; samplesPerSecond = 51116.5
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.43095093 * 2560; err = 0.41640625 * 2560; time = 0.0509s; samplesPerSecond = 50327.5
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.41503601 * 2560; err = 0.40078125 * 2560; time = 0.0499s; samplesPerSecond = 51272.2
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.38912964 * 2560; err = 0.41132812 * 2560; time = 0.0496s; samplesPerSecond = 51596.4
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.41208496 * 2560; err = 0.42226562 * 2560; time = 0.0500s; samplesPerSecond = 51193.7
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.39965820 * 2560; err = 0.40664062 * 2560; time = 0.0517s; samplesPerSecond = 49509.4
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42728577 * 2560; err = 0.42617187 * 2560; time = 0.0499s; samplesPerSecond = 51264.3
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.41336060 * 2560; err = 0.42304687 * 2560; time = 0.0506s; samplesPerSecond = 50576.3
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.33195801 * 2560; err = 0.39960937 * 2560; time = 0.0502s; samplesPerSecond = 51019.1
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.28579712 * 2560; err = 0.38671875 * 2560; time = 0.0501s; samplesPerSecond = 51049.8
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.34128113 * 2560; err = 0.40937500 * 2560; time = 0.0494s; samplesPerSecond = 51779.8
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.32663879 * 2560; err = 0.39648438 * 2560; time = 0.0502s; samplesPerSecond = 50961.1
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.21426086 * 2560; err = 0.37187500 * 2560; time = 0.0492s; samplesPerSecond = 51981.7
12/31/2017 19:37:04:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23750000 * 2560; err = 0.37382813 * 2560; time = 0.0498s; samplesPerSecond = 51400.8
12/31/2017 19:37:05:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.29968262 * 2560; err = 0.39062500 * 2560; time = 0.0520s; samplesPerSecond = 49199.6
12/31/2017 19:37:05:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.21239929 * 2560; err = 0.37382813 * 2560; time = 0.0503s; samplesPerSecond = 50894.2
12/31/2017 19:37:05:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20531006 * 2560; err = 0.36796875 * 2560; time = 0.0510s; samplesPerSecond = 50163.3
12/31/2017 19:37:05:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23578796 * 2560; err = 0.37187500 * 2560; time = 0.0500s; samplesPerSecond = 51174.8
12/31/2017 19:37:05:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.25570068 * 2560; err = 0.37968750 * 2560; time = 0.0450s; samplesPerSecond = 56857.4
12/31/2017 19:37:05: Finished Epoch[ 1 of 2]: [Training] ce = 1.62944050 * 81920; err = 0.46015625 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=2.54224s
12/31/2017 19:37:05: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre1/cntkSpeech.1'

12/31/2017 19:37:05: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

12/31/2017 19:37:05: Starting minibatch loop.
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.23324184 * 2560; err = 0.38164063 * 2560; time = 0.0513s; samplesPerSecond = 49860.0
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.20339603 * 2560; err = 0.37226562 * 2560; time = 0.0503s; samplesPerSecond = 50928.9
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.28589058 * 2560; err = 0.37773438 * 2560; time = 0.0499s; samplesPerSecond = 51324.6
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.23064728 * 2560; err = 0.37773438 * 2560; time = 0.0499s; samplesPerSecond = 51351.5
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.18116455 * 2560; err = 0.35585937 * 2560; time = 0.0499s; samplesPerSecond = 51324.6
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.28143578 * 2560; err = 0.37929687 * 2560; time = 0.0502s; samplesPerSecond = 50969.0
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.22316818 * 2560; err = 0.37226562 * 2560; time = 0.0499s; samplesPerSecond = 51294.0
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.17917328 * 2560; err = 0.36679688 * 2560; time = 0.0497s; samplesPerSecond = 51494.7
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.23667755 * 2560; err = 0.36210938 * 2560; time = 0.0522s; samplesPerSecond = 49001.4
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18298416 * 2560; err = 0.37382813 * 2560; time = 0.0501s; samplesPerSecond = 51109.4
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.20064545 * 2560; err = 0.36562500 * 2560; time = 0.0494s; samplesPerSecond = 51796.6
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18663025 * 2560; err = 0.35156250 * 2560; time = 0.0500s; samplesPerSecond = 51232.3
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.16763000 * 2560; err = 0.35546875 * 2560; time = 0.0499s; samplesPerSecond = 51282.3
12/31/2017 19:37:05:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.13098907 * 2560; err = 0.35039063 * 2560; time = 0.0500s; samplesPerSecond = 51234.2
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.09752045 * 2560; err = 0.32265625 * 2560; time = 0.0497s; samplesPerSecond = 51532.3
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09747925 * 2560; err = 0.33710937 * 2560; time = 0.0506s; samplesPerSecond = 50621.2
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.19010925 * 2560; err = 0.35625000 * 2560; time = 0.0503s; samplesPerSecond = 50937.5
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.16201630 * 2560; err = 0.36015625 * 2560; time = 0.0502s; samplesPerSecond = 50989.7
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.11661987 * 2560; err = 0.34375000 * 2560; time = 0.0496s; samplesPerSecond = 51640.8
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.11697693 * 2560; err = 0.34765625 * 2560; time = 0.0497s; samplesPerSecond = 51508.6
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.09923859 * 2560; err = 0.33125000 * 2560; time = 0.0502s; samplesPerSecond = 50983.6
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.12601929 * 2560; err = 0.33789062 * 2560; time = 0.0499s; samplesPerSecond = 51298.9
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.14013977 * 2560; err = 0.35429688 * 2560; time = 0.0499s; samplesPerSecond = 51301.0
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.29293518 * 2560; err = 0.38867188 * 2560; time = 0.0499s; samplesPerSecond = 51332.3
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.18086853 * 2560; err = 0.35898438 * 2560; time = 0.0499s; samplesPerSecond = 51319.1
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.14109802 * 2560; err = 0.36171875 * 2560; time = 0.0694s; samplesPerSecond = 36891.1
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.14666138 * 2560; err = 0.34492187 * 2560; time = 0.0529s; samplesPerSecond = 48377.9
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.15356140 * 2560; err = 0.35000000 * 2560; time = 0.0522s; samplesPerSecond = 49051.4
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.07240295 * 2560; err = 0.33867188 * 2560; time = 0.0529s; samplesPerSecond = 48438.5
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.08450623 * 2560; err = 0.33007812 * 2560; time = 0.0520s; samplesPerSecond = 49203.5
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.07801514 * 2560; err = 0.33164063 * 2560; time = 0.0517s; samplesPerSecond = 49483.6
12/31/2017 19:37:06:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.06560974 * 2560; err = 0.33281250 * 2560; time = 0.0474s; samplesPerSecond = 54018.4
12/31/2017 19:37:06: Finished Epoch[ 2 of 2]: [Training] ce = 1.16517038 * 81920; err = 0.35534668 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=1.64057s
12/31/2017 19:37:06: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre1/cntkSpeech'

12/31/2017 19:37:06: Action "train" complete.


12/31/2017 19:37:06: ##############################################################################
12/31/2017 19:37:06: #                                                                            #
12/31/2017 19:37:06: # addLayer2 command (edit action)                                            #
12/31/2017 19:37:06: #                                                                            #
12/31/2017 19:37:06: ##############################################################################


12/31/2017 19:37:07: Action "edit" complete.


12/31/2017 19:37:07: ##############################################################################
12/31/2017 19:37:07: #                                                                            #
12/31/2017 19:37:07: # dptPre2 command (train action)                                             #
12/31/2017 19:37:07: #                                                                            #
12/31/2017 19:37:07: ##############################################################################

12/31/2017 19:37:07: 
Starting from checkpoint. Loading network from '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre2/cntkSpeech.0'.
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
12/31/2017 19:37:07: 
Model has 24 nodes. Using GPU 0.

12/31/2017 19:37:07: Training criterion:   ce = CrossEntropyWithSoftmax
12/31/2017 19:37:07: Evaluation criterion: err = ClassificationError

12/31/2017 19:37:07: Training 516740 parameters in 6 out of 6 parameter tensors and 15 nodes with gradient:

12/31/2017 19:37:07: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
12/31/2017 19:37:07: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
12/31/2017 19:37:07: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
12/31/2017 19:37:07: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
12/31/2017 19:37:07: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
12/31/2017 19:37:07: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

12/31/2017 19:37:07: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/31/2017 19:37:07: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

12/31/2017 19:37:07: Starting minibatch loop.
12/31/2017 19:37:07:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 4.65436707 * 2560; err = 0.81132812 * 2560; time = 0.0660s; samplesPerSecond = 38802.2
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.87611237 * 2560; err = 0.70664063 * 2560; time = 0.0657s; samplesPerSecond = 38980.8
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.30106964 * 2560; err = 0.59882813 * 2560; time = 0.0638s; samplesPerSecond = 40102.4
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.96660919 * 2560; err = 0.52539062 * 2560; time = 0.0635s; samplesPerSecond = 40320.6
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.74655914 * 2560; err = 0.48398438 * 2560; time = 0.0638s; samplesPerSecond = 40123.1
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.62835999 * 2560; err = 0.46210937 * 2560; time = 0.0632s; samplesPerSecond = 40478.5
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.57696533 * 2560; err = 0.45468750 * 2560; time = 0.0633s; samplesPerSecond = 40457.5
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.47685089 * 2560; err = 0.42695312 * 2560; time = 0.0630s; samplesPerSecond = 40615.5
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.39075470 * 2560; err = 0.40625000 * 2560; time = 0.0640s; samplesPerSecond = 39970.2
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41970062 * 2560; err = 0.42421875 * 2560; time = 0.0635s; samplesPerSecond = 40317.6
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.41201630 * 2560; err = 0.43476562 * 2560; time = 0.0631s; samplesPerSecond = 40546.0
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.38169403 * 2560; err = 0.41562500 * 2560; time = 0.0634s; samplesPerSecond = 40390.3
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.34602051 * 2560; err = 0.41093750 * 2560; time = 0.0632s; samplesPerSecond = 40495.9
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.38628082 * 2560; err = 0.39843750 * 2560; time = 0.0631s; samplesPerSecond = 40539.2
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.32571716 * 2560; err = 0.39023438 * 2560; time = 0.0626s; samplesPerSecond = 40892.6
12/31/2017 19:37:08:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.31520691 * 2560; err = 0.39140625 * 2560; time = 0.0629s; samplesPerSecond = 40685.0
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.26128540 * 2560; err = 0.37539062 * 2560; time = 0.0637s; samplesPerSecond = 40215.5
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28179016 * 2560; err = 0.38593750 * 2560; time = 0.0648s; samplesPerSecond = 39506.0
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.29704285 * 2560; err = 0.39062500 * 2560; time = 0.0635s; samplesPerSecond = 40305.0
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.28553772 * 2560; err = 0.39218750 * 2560; time = 0.0634s; samplesPerSecond = 40395.2
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.26621399 * 2560; err = 0.38828125 * 2560; time = 0.0640s; samplesPerSecond = 39988.6
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.21440430 * 2560; err = 0.36796875 * 2560; time = 0.0631s; samplesPerSecond = 40593.5
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.21631470 * 2560; err = 0.36992188 * 2560; time = 0.0636s; samplesPerSecond = 40222.5
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.24478760 * 2560; err = 0.37812500 * 2560; time = 0.0632s; samplesPerSecond = 40519.6
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.22629700 * 2560; err = 0.37734375 * 2560; time = 0.0637s; samplesPerSecond = 40183.1
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.15057373 * 2560; err = 0.34765625 * 2560; time = 0.0633s; samplesPerSecond = 40439.9
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.16724548 * 2560; err = 0.35156250 * 2560; time = 0.0632s; samplesPerSecond = 40530.8
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.22637939 * 2560; err = 0.36601563 * 2560; time = 0.0641s; samplesPerSecond = 39929.1
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.16267090 * 2560; err = 0.35976562 * 2560; time = 0.0663s; samplesPerSecond = 38603.3
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.16969299 * 2560; err = 0.35820313 * 2560; time = 0.0633s; samplesPerSecond = 40411.5
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.16425781 * 2560; err = 0.35195312 * 2560; time = 0.0635s; samplesPerSecond = 40340.6
12/31/2017 19:37:09:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.17123108 * 2560; err = 0.35234375 * 2560; time = 0.0588s; samplesPerSecond = 43501.4
12/31/2017 19:37:09: Finished Epoch[ 1 of 2]: [Training] ce = 1.52218781 * 81920; err = 0.42672119 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=2.78829s
12/31/2017 19:37:09: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre2/cntkSpeech.1'

12/31/2017 19:37:10: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

12/31/2017 19:37:10: Starting minibatch loop.
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.14802532 * 2560; err = 0.35117188 * 2560; time = 0.0649s; samplesPerSecond = 39421.6
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.17314568 * 2560; err = 0.36015625 * 2560; time = 0.0638s; samplesPerSecond = 40156.7
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.23122158 * 2560; err = 0.37265625 * 2560; time = 0.0640s; samplesPerSecond = 40011.0
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.17969055 * 2560; err = 0.35898438 * 2560; time = 0.0637s; samplesPerSecond = 40216.1
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.13304825 * 2560; err = 0.34843750 * 2560; time = 0.0716s; samplesPerSecond = 35753.8
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.22006378 * 2560; err = 0.36953125 * 2560; time = 0.0640s; samplesPerSecond = 39995.1
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.14098816 * 2560; err = 0.34726563 * 2560; time = 0.0632s; samplesPerSecond = 40503.7
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.12340546 * 2560; err = 0.34921875 * 2560; time = 0.0628s; samplesPerSecond = 40774.7
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.14668121 * 2560; err = 0.33945313 * 2560; time = 0.0685s; samplesPerSecond = 37370.4
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.12819672 * 2560; err = 0.34960938 * 2560; time = 0.0635s; samplesPerSecond = 40303.5
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.14891510 * 2560; err = 0.34687500 * 2560; time = 0.0634s; samplesPerSecond = 40392.6
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.13158951 * 2560; err = 0.34296875 * 2560; time = 0.0633s; samplesPerSecond = 40416.0
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.11186981 * 2560; err = 0.33671875 * 2560; time = 0.0624s; samplesPerSecond = 41021.0
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.06695404 * 2560; err = 0.32382813 * 2560; time = 0.0648s; samplesPerSecond = 39529.1
12/31/2017 19:37:10:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.05429382 * 2560; err = 0.30937500 * 2560; time = 0.0631s; samplesPerSecond = 40559.0
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.06546631 * 2560; err = 0.32851562 * 2560; time = 0.0628s; samplesPerSecond = 40776.0
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.13989563 * 2560; err = 0.34335938 * 2560; time = 0.0626s; samplesPerSecond = 40897.6
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.13786621 * 2560; err = 0.35976562 * 2560; time = 0.0635s; samplesPerSecond = 40296.2
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.07601318 * 2560; err = 0.33125000 * 2560; time = 0.0634s; samplesPerSecond = 40404.8
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.06908112 * 2560; err = 0.33476563 * 2560; time = 0.0645s; samplesPerSecond = 39711.0
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.05689087 * 2560; err = 0.32656250 * 2560; time = 0.0631s; samplesPerSecond = 40572.2
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.08632202 * 2560; err = 0.32890625 * 2560; time = 0.0632s; samplesPerSecond = 40511.3
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.11229401 * 2560; err = 0.34375000 * 2560; time = 0.0633s; samplesPerSecond = 40441.1
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.12421875 * 2560; err = 0.34960938 * 2560; time = 0.0663s; samplesPerSecond = 38600.8
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.08749695 * 2560; err = 0.33593750 * 2560; time = 0.0644s; samplesPerSecond = 39781.5
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.06223145 * 2560; err = 0.32812500 * 2560; time = 0.0633s; samplesPerSecond = 40433.7
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.06559448 * 2560; err = 0.32500000 * 2560; time = 0.0635s; samplesPerSecond = 40297.1
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.10242615 * 2560; err = 0.33007812 * 2560; time = 0.0633s; samplesPerSecond = 40474.3
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.02162476 * 2560; err = 0.32539062 * 2560; time = 0.0631s; samplesPerSecond = 40542.3
12/31/2017 19:37:11:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.06609802 * 2560; err = 0.32617188 * 2560; time = 0.0683s; samplesPerSecond = 37464.0
12/31/2017 19:37:12:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.05944214 * 2560; err = 0.32304688 * 2560; time = 0.0633s; samplesPerSecond = 40448.5
12/31/2017 19:37:12:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.05012207 * 2560; err = 0.33242187 * 2560; time = 0.0590s; samplesPerSecond = 43353.5
12/31/2017 19:37:12: Finished Epoch[ 2 of 2]: [Training] ce = 1.11003666 * 81920; err = 0.33996582 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=2.05728s
12/31/2017 19:37:12: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/Pre2/cntkSpeech'

12/31/2017 19:37:12: Action "train" complete.


12/31/2017 19:37:12: ##############################################################################
12/31/2017 19:37:12: #                                                                            #
12/31/2017 19:37:12: # addLayer3 command (edit action)                                            #
12/31/2017 19:37:12: #                                                                            #
12/31/2017 19:37:12: ##############################################################################


12/31/2017 19:37:12: Action "edit" complete.


12/31/2017 19:37:12: ##############################################################################
12/31/2017 19:37:12: #                                                                            #
12/31/2017 19:37:12: # speechTrain command (train action)                                         #
12/31/2017 19:37:12: #                                                                            #
12/31/2017 19:37:12: ##############################################################################

12/31/2017 19:37:12: 
Starting from checkpoint. Loading network from '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.0'.
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
12/31/2017 19:37:12: 
Model has 29 nodes. Using GPU 0.

12/31/2017 19:37:12: Training criterion:   ce = CrossEntropyWithSoftmax
12/31/2017 19:37:12: Evaluation criterion: err = ClassificationError

12/31/2017 19:37:12: Training 779396 parameters in 8 out of 8 parameter tensors and 20 nodes with gradient:

12/31/2017 19:37:12: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
12/31/2017 19:37:12: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
12/31/2017 19:37:12: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
12/31/2017 19:37:12: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
12/31/2017 19:37:12: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
12/31/2017 19:37:12: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
12/31/2017 19:37:12: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
12/31/2017 19:37:12: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

12/31/2017 19:37:12: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/31/2017 19:37:12: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

12/31/2017 19:37:13: Starting minibatch loop.
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: ce = 3.98617706 * 2560; err = 0.81132812 * 2560; time = 0.0775s; samplesPerSecond = 33015.2
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.65327377 * 2560; err = 0.64609375 * 2560; time = 0.0739s; samplesPerSecond = 34622.8
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.04344254 * 2560; err = 0.54882812 * 2560; time = 0.0749s; samplesPerSecond = 34184.5
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.75020447 * 2560; err = 0.47851562 * 2560; time = 0.0893s; samplesPerSecond = 28667.4
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: ce = 1.57851944 * 2560; err = 0.44960937 * 2560; time = 0.0716s; samplesPerSecond = 35778.2
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.47935410 * 2560; err = 0.42226562 * 2560; time = 0.0862s; samplesPerSecond = 29681.6
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.43710175 * 2560; err = 0.41054687 * 2560; time = 0.0701s; samplesPerSecond = 36497.6
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.36245575 * 2560; err = 0.39609375 * 2560; time = 0.0894s; samplesPerSecond = 28626.8
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: ce = 1.27734070 * 2560; err = 0.37539062 * 2560; time = 0.0694s; samplesPerSecond = 36866.6
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.30455475 * 2560; err = 0.39843750 * 2560; time = 0.0687s; samplesPerSecond = 37243.1
12/31/2017 19:37:13:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.28724823 * 2560; err = 0.38984375 * 2560; time = 0.0687s; samplesPerSecond = 37252.7
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.27779541 * 2560; err = 0.38398437 * 2560; time = 0.0727s; samplesPerSecond = 35214.0
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: ce = 1.24093323 * 2560; err = 0.38359375 * 2560; time = 0.0838s; samplesPerSecond = 30554.9
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.31192780 * 2560; err = 0.38750000 * 2560; time = 0.0678s; samplesPerSecond = 37736.3
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.25088348 * 2560; err = 0.36796875 * 2560; time = 0.0679s; samplesPerSecond = 37687.7
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.26782379 * 2560; err = 0.38085938 * 2560; time = 0.0679s; samplesPerSecond = 37686.7
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: ce = 1.19906921 * 2560; err = 0.35820313 * 2560; time = 0.0705s; samplesPerSecond = 36333.8
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.20673828 * 2560; err = 0.36679688 * 2560; time = 0.0685s; samplesPerSecond = 37399.3
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.21287537 * 2560; err = 0.36953125 * 2560; time = 0.0684s; samplesPerSecond = 37407.9
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.20243225 * 2560; err = 0.37617187 * 2560; time = 0.0687s; samplesPerSecond = 37281.2
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: ce = 1.20143738 * 2560; err = 0.37460938 * 2560; time = 0.0683s; samplesPerSecond = 37485.2
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.13604126 * 2560; err = 0.34414062 * 2560; time = 0.0685s; samplesPerSecond = 37376.5
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.14999695 * 2560; err = 0.34687500 * 2560; time = 0.0678s; samplesPerSecond = 37779.2
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.18746033 * 2560; err = 0.35703125 * 2560; time = 0.0696s; samplesPerSecond = 36755.8
12/31/2017 19:37:14:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: ce = 1.17141724 * 2560; err = 0.36210938 * 2560; time = 0.0682s; samplesPerSecond = 37542.9
12/31/2017 19:37:15:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.08553772 * 2560; err = 0.33867188 * 2560; time = 0.0683s; samplesPerSecond = 37474.5
12/31/2017 19:37:15:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.11324768 * 2560; err = 0.34335938 * 2560; time = 0.0678s; samplesPerSecond = 37732.7
12/31/2017 19:37:15:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.17917480 * 2560; err = 0.35351562 * 2560; time = 0.0679s; samplesPerSecond = 37694.2
12/31/2017 19:37:15:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: ce = 1.11235657 * 2560; err = 0.34375000 * 2560; time = 0.0686s; samplesPerSecond = 37342.9
12/31/2017 19:37:15:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.13317871 * 2560; err = 0.34257813 * 2560; time = 0.0684s; samplesPerSecond = 37443.7
12/31/2017 19:37:15:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.12384949 * 2560; err = 0.34062500 * 2560; time = 0.0679s; samplesPerSecond = 37684.9
12/31/2017 19:37:15:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.12303772 * 2560; err = 0.34257813 * 2560; time = 0.0662s; samplesPerSecond = 38687.6
12/31/2017 19:37:15: Finished Epoch[ 1 of 4]: [Training] ce = 1.40771523 * 81920; err = 0.40285645 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=3.045s
12/31/2017 19:37:15: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.1'

12/31/2017 19:37:15: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

12/31/2017 19:37:15: Starting minibatch loop.
12/31/2017 19:37:15:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.50690136 * 5120; err = 0.41445312 * 5120; time = 0.1041s; samplesPerSecond = 49181.3
12/31/2017 19:37:15:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.43394032 * 5120; err = 0.40312500 * 5120; time = 0.1018s; samplesPerSecond = 50299.1
12/31/2017 19:37:15:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.19498672 * 5120; err = 0.36425781 * 5120; time = 0.1017s; samplesPerSecond = 50335.9
12/31/2017 19:37:15:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.13462753 * 5120; err = 0.34609375 * 5120; time = 0.1020s; samplesPerSecond = 50206.9
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.12732811 * 5120; err = 0.34101562 * 5120; time = 0.1015s; samplesPerSecond = 50456.3
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.13973351 * 5120; err = 0.34941406 * 5120; time = 0.1017s; samplesPerSecond = 50349.1
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.09274750 * 5120; err = 0.33613281 * 5120; time = 0.1019s; samplesPerSecond = 50241.4
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.07956161 * 5120; err = 0.33359375 * 5120; time = 0.1019s; samplesPerSecond = 50222.3
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.14393692 * 5120; err = 0.35195312 * 5120; time = 0.1015s; samplesPerSecond = 50456.1
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.06811905 * 5120; err = 0.33457031 * 5120; time = 0.1022s; samplesPerSecond = 50111.1
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.06341400 * 5120; err = 0.33496094 * 5120; time = 0.1033s; samplesPerSecond = 49557.1
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.12287140 * 5120; err = 0.35117188 * 5120; time = 0.1011s; samplesPerSecond = 50655.2
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.09922638 * 5120; err = 0.34550781 * 5120; time = 0.1023s; samplesPerSecond = 50037.5
12/31/2017 19:37:16:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.08422241 * 5120; err = 0.32832031 * 5120; time = 0.1010s; samplesPerSecond = 50707.6
12/31/2017 19:37:17:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.03713837 * 5120; err = 0.32539062 * 5120; time = 0.1081s; samplesPerSecond = 47363.5
12/31/2017 19:37:17:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.05823059 * 5120; err = 0.32636719 * 5120; time = 0.1001s; samplesPerSecond = 51166.9
12/31/2017 19:37:17: Finished Epoch[ 2 of 4]: [Training] ce = 1.14918661 * 81920; err = 0.34914551 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=1.65134s
12/31/2017 19:37:17: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.2'

12/31/2017 19:37:17: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

12/31/2017 19:37:17: Starting minibatch loop.
12/31/2017 19:37:17:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.11066446 * 5120; err = 0.34218750 * 5120; time = 0.1036s; samplesPerSecond = 49442.1
12/31/2017 19:37:17:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.10631495 * 5120; err = 0.34511719 * 5120; time = 0.1033s; samplesPerSecond = 49584.8
12/31/2017 19:37:17:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.09502583 * 5120; err = 0.34394531 * 5120; time = 0.1034s; samplesPerSecond = 49507.4
12/31/2017 19:37:17:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.14925079 * 5120; err = 0.34570312 * 5120; time = 0.1028s; samplesPerSecond = 49783.9
12/31/2017 19:37:17:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.13400726 * 5120; err = 0.34199219 * 5120; time = 0.1076s; samplesPerSecond = 47594.6
12/31/2017 19:37:17:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.07996712 * 5120; err = 0.33652344 * 5120; time = 0.1033s; samplesPerSecond = 49554.1
12/31/2017 19:37:17:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.07324677 * 5120; err = 0.33085938 * 5120; time = 0.1021s; samplesPerSecond = 50141.3
12/31/2017 19:37:18:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.06941147 * 5120; err = 0.33046875 * 5120; time = 0.1051s; samplesPerSecond = 48712.6
12/31/2017 19:37:18:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.02541656 * 5120; err = 0.31171875 * 5120; time = 0.1024s; samplesPerSecond = 49993.2
12/31/2017 19:37:18:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.04684372 * 5120; err = 0.31835938 * 5120; time = 0.1028s; samplesPerSecond = 49818.0
12/31/2017 19:37:18:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.05219879 * 5120; err = 0.33691406 * 5120; time = 0.1021s; samplesPerSecond = 50160.5
12/31/2017 19:37:18:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.07827148 * 5120; err = 0.33300781 * 5120; time = 0.1029s; samplesPerSecond = 49776.0
12/31/2017 19:37:18:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05986786 * 5120; err = 0.32187500 * 5120; time = 0.1017s; samplesPerSecond = 50343.7
12/31/2017 19:37:18:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02186890 * 5120; err = 0.31972656 * 5120; time = 0.1039s; samplesPerSecond = 49290.0
12/31/2017 19:37:18:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.04579926 * 5120; err = 0.32832031 * 5120; time = 0.1027s; samplesPerSecond = 49875.9
12/31/2017 19:37:18:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.02374115 * 5120; err = 0.32285156 * 5120; time = 0.0931s; samplesPerSecond = 55020.5
12/31/2017 19:37:18: Finished Epoch[ 3 of 4]: [Training] ce = 1.07324352 * 81920; err = 0.33184814 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=1.65755s
12/31/2017 19:37:18: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.3'

12/31/2017 19:37:18: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

12/31/2017 19:37:18: Starting minibatch loop.
12/31/2017 19:37:19:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.02762766 * 5120; err = 0.31425781 * 5120; time = 0.1041s; samplesPerSecond = 49171.3
12/31/2017 19:37:19:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.04846405 * 4926; err = 0.32866423 * 4926; time = 0.2995s; samplesPerSecond = 16448.1
12/31/2017 19:37:19:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.01424675 * 5120; err = 0.32382813 * 5120; time = 0.1030s; samplesPerSecond = 49723.8
12/31/2017 19:37:19:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 0.99905319 * 5120; err = 0.31582031 * 5120; time = 0.1098s; samplesPerSecond = 46622.0
12/31/2017 19:37:19:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 0.99530144 * 5120; err = 0.31191406 * 5120; time = 0.1063s; samplesPerSecond = 48165.0
12/31/2017 19:37:19:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00318260 * 5120; err = 0.32304688 * 5120; time = 0.1028s; samplesPerSecond = 49803.2
12/31/2017 19:37:19:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.01893272 * 5120; err = 0.31933594 * 5120; time = 0.1039s; samplesPerSecond = 49254.7
12/31/2017 19:37:20:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 0.99314346 * 5120; err = 0.31054688 * 5120; time = 0.1020s; samplesPerSecond = 50180.3
12/31/2017 19:37:20:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99949417 * 5120; err = 0.30625000 * 5120; time = 0.1024s; samplesPerSecond = 50007.6
12/31/2017 19:37:20:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.99980240 * 5120; err = 0.31035156 * 5120; time = 0.1014s; samplesPerSecond = 50506.2
12/31/2017 19:37:20:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.02506561 * 5120; err = 0.31738281 * 5120; time = 0.1025s; samplesPerSecond = 49960.9
12/31/2017 19:37:20:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.03906784 * 5120; err = 0.32265625 * 5120; time = 0.1029s; samplesPerSecond = 49766.8
12/31/2017 19:37:20:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.97148972 * 5120; err = 0.29550781 * 5120; time = 0.1024s; samplesPerSecond = 49983.9
12/31/2017 19:37:20:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97292023 * 5120; err = 0.30605469 * 5120; time = 0.1020s; samplesPerSecond = 50197.4
12/31/2017 19:37:20:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.98358307 * 5120; err = 0.31269531 * 5120; time = 0.1022s; samplesPerSecond = 50119.5
12/31/2017 19:37:20:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.98013763 * 5120; err = 0.29785156 * 5120; time = 0.0974s; samplesPerSecond = 52540.8
12/31/2017 19:37:20: Finished Epoch[ 4 of 4]: [Training] ce = 1.00425825 * 81920; err = 0.31357422 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=1.86366s
12/31/2017 19:37:20: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech'

12/31/2017 19:37:20: Action "train" complete.


12/31/2017 19:37:20: ##############################################################################
12/31/2017 19:37:20: #                                                                            #
12/31/2017 19:37:20: # sequenceTrain command (train action)                                       #
12/31/2017 19:37:20: #                                                                            #
12/31/2017 19:37:20: ##############################################################################

12/31/2017 19:37:20: 
Creating virgin network.
Load: Loading model file: /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech
Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ClassificationError()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *8]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *8]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *8], [363 x 1], [363 x 1] -> [363 x *8]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *8] -> [512 x *8]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *8], [512 x 1] -> [512 x 1 x *8]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *8], [512 x 1] -> [512 x 1 x *8]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *8], [512 x 1] -> [512 x 1 x *8]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *8] -> [512 x 1 x *8]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *8] -> [132 x 1 x *8]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *8], [132 x 1] -> [132 x 1 x *8]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *8], [132 x 1 x *8] -> [1]
Validating --> err = ClassificationError (labels, OL.z) : [132 x *8], [132 x 1 x *8] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *8], [132 x 1] -> [132 x 1 x *8]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.




Post-processing network complete.

CloneFunction: (features : InputValue) -> [
    netEval = OL.z : Plus
    scaledLogLikelihood = scaledLogLikelihood : Minus
]
clonedmodel.OL.t.inputs[0] = OL.W (174) ==>  clonedmodel.OL.W (183)
clonedmodel.OL.t.inputs[1] = HL3.y (168) ==>  clonedmodel.HL3.y (203)
clonedmodel.OL.z.inputs[0] = OL.t (173) ==>  clonedmodel.OL.t (182)
clonedmodel.OL.z.inputs[1] = OL.b (172) ==>  clonedmodel.OL.b (181)
clonedmodel.scaledLogLikelihood.inputs[0] = OL.z (175) ==>  clonedmodel.OL.z (184)
clonedmodel.scaledLogLikelihood.inputs[1] = logPrior (171) ==>  clonedmodel.logPrior (205)
clonedmodel.featNorm.inputs[0] = features (151) ==>  features (180)
clonedmodel.featNorm.inputs[1] = globalMean (153) ==>  clonedmodel.globalMean (188)
clonedmodel.featNorm.inputs[2] = globalInvStd (152) ==>  clonedmodel.globalInvStd (187)
clonedmodel.HL1.t.inputs[0] = HL1.W (157) ==>  clonedmodel.HL1.W (192)
clonedmodel.HL1.t.inputs[1] = featNorm (150) ==>  clonedmodel.featNorm (186)
clonedmodel.HL1.y.inputs[0] = HL1.z (159) ==>  clonedmodel.HL1.z (194)
clonedmodel.HL1.z.inputs[0] = HL1.t (156) ==>  clonedmodel.HL1.t (191)
clonedmodel.HL1.z.inputs[1] = HL1.b (155) ==>  clonedmodel.HL1.b (190)
clonedmodel.HL2.t.inputs[0] = HL2.W (162) ==>  clonedmodel.HL2.W (197)
clonedmodel.HL2.t.inputs[1] = HL1.y (158) ==>  clonedmodel.HL1.y (193)
clonedmodel.HL2.y.inputs[0] = HL2.z (164) ==>  clonedmodel.HL2.z (199)
clonedmodel.HL2.z.inputs[0] = HL2.t (161) ==>  clonedmodel.HL2.t (196)
clonedmodel.HL2.z.inputs[1] = HL2.b (160) ==>  clonedmodel.HL2.b (195)
clonedmodel.HL3.t.inputs[0] = HL3.W (167) ==>  clonedmodel.HL3.W (202)
clonedmodel.HL3.t.inputs[1] = HL2.y (163) ==>  clonedmodel.HL2.y (198)
clonedmodel.HL3.y.inputs[0] = HL3.z (169) ==>  clonedmodel.HL3.z (204)
clonedmodel.HL3.z.inputs[0] = HL3.t (166) ==>  clonedmodel.HL3.t (201)
clonedmodel.HL3.z.inputs[1] = HL3.b (165) ==>  clonedmodel.HL3.b (200)
clonedmodel.logPrior.inputs[0] = globalPrior (154) ==>  clonedmodel.globalPrior (189)
CloneFunction: Cloned 25 nodes and relinked 25 inputs.
12/31/2017 19:37:20: Reading files
 /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/CY2SCH010061231_1369712653.numden.lats.symlist 
 /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/model.overalltying 
 /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list 
 /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/model.transprob 
simplesenonehmm: reading '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/model.overalltying', '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list', '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read

Post-processing network...

4 roots:
	Err = ClassificationError()
	clonedmodel.scaledLogLikelihood = Minus()
	cr = SequenceWithLattice()
	latticeAxis = DynamicAxis()

Validating network. 31 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> clonedmodel.OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> clonedmodel.HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> clonedmodel.HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> clonedmodel.HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> clonedmodel.globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> clonedmodel.globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> clonedmodel.featNorm = PerDimMeanVarNormalization (features, clonedmodel.globalMean, clonedmodel.globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> clonedmodel.HL1.t = Times (clonedmodel.HL1.W, clonedmodel.featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> clonedmodel.HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> clonedmodel.HL1.z = Plus (clonedmodel.HL1.t, clonedmodel.HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> clonedmodel.HL1.y = Sigmoid (clonedmodel.HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.HL2.t = Times (clonedmodel.HL2.W, clonedmodel.HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> clonedmodel.HL2.z = Plus (clonedmodel.HL2.t, clonedmodel.HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> clonedmodel.HL2.y = Sigmoid (clonedmodel.HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.HL3.t = Times (clonedmodel.HL3.W, clonedmodel.HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> clonedmodel.HL3.z = Plus (clonedmodel.HL3.t, clonedmodel.HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> clonedmodel.HL3.y = Sigmoid (clonedmodel.HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> clonedmodel.OL.t = Times (clonedmodel.OL.W, clonedmodel.HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> clonedmodel.OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> clonedmodel.OL.z = Plus (clonedmodel.OL.t, clonedmodel.OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> Err = ClassificationError (labels, clonedmodel.OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> clonedmodel.globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> clonedmodel.logPrior = Log (clonedmodel.globalPrior) : [132 x 1] -> [132 x 1]
Validating --> clonedmodel.scaledLogLikelihood = Minus (clonedmodel.OL.z, clonedmodel.logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> lattice = InputValue() :  -> [1 x latticeAxis]
Validating --> cr = SequenceWithLattice (labels, clonedmodel.OL.z, clonedmodel.scaledLogLikelihood, lattice) : [132 x *7], [132 x 1 x *7], [132 x 1 x *7], [1 x latticeAxis] -> [1]
Validating --> latticeAxis = DynamicAxis() :  -> [1 x 1 x latticeAxis]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.




Post-processing network complete.

Reading script file /tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/glob_0000.scp ... 948 entries
HTKDeserializer: selected '948' utterances grouped into '3' chunks, average chunk size: 316.0 utterances, 84244.7 frames (for I/O: 316.0 utterances, 84244.7 frames)
HTKDeserializer: determined feature kind as '33'-dimensional 'USER' with frame shift 10.0 ms
Total (133) state names in state list '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/state.list'
MLFDeserializer: '948' utterances with '252734' frames
Current working directory is '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData' 
Reading lattice index file '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/TestData/latticeIndex.txt' ...
LatticeDeserializer: '923' sequences
12/31/2017 19:37:21: 
Model has 31 nodes. Using GPU 0.

12/31/2017 19:37:21: Training criterion:   cr = SequenceWithLattice
12/31/2017 19:37:21: Evaluation criterion: Err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 6 are aliased.
	clonedmodel.HL3.t (gradient) reuses clonedmodel.HL3.z (gradient)
	clonedmodel.HL2.t (gradient) reuses clonedmodel.HL2.z (gradient)
	clonedmodel.OL.t (gradient) reuses clonedmodel.OL.z (gradient)

Memory Sharing: Out of 52 matrices, 30 are shared as 7, and 22 are not shared.

Here are the ones that share memory:
	{ clonedmodel.HL1.b : [512 x 1] (gradient)
	  clonedmodel.HL1.y : [512 x 1 x *7] }
	{ clonedmodel.HL1.t : [512 x *7]
	  clonedmodel.HL1.t : [512 x *7] (gradient)
	  clonedmodel.HL1.y : [512 x 1 x *7] (gradient)
	  clonedmodel.HL2.y : [512 x 1 x *7] (gradient)
	  clonedmodel.HL2.z : [512 x 1 x *7]
	  clonedmodel.HL3.y : [512 x 1 x *7] (gradient)
	  clonedmodel.scaledLogLikelihood : [132 x 1 x *7] }
	{ clonedmodel.HL1.W : [512 x 363] (gradient)
	  clonedmodel.HL1.z : [512 x 1 x *7]
	  clonedmodel.HL1.z : [512 x 1 x *7] (gradient)
	  clonedmodel.HL2.t : [512 x 1 x *7]
	  clonedmodel.HL2.t : [512 x 1 x *7] (gradient)
	  clonedmodel.HL2.z : [512 x 1 x *7] (gradient)
	  clonedmodel.HL3.t : [512 x 1 x *7] (gradient)
	  clonedmodel.HL3.z : [512 x 1 x *7]
	  clonedmodel.HL3.z : [512 x 1 x *7] (gradient)
	  clonedmodel.scaledLogLikelihood : [132 x 1 x *7] (gradient) }
	{ clonedmodel.HL3.W : [512 x 512] (gradient)
	  clonedmodel.OL.t : [132 x 1 x *7]
	  clonedmodel.OL.t : [132 x 1 x *7] (gradient)
	  clonedmodel.OL.z : [132 x 1 x *7] (gradient) }
	{ clonedmodel.OL.W : [132 x 512] (gradient)
	  clonedmodel.OL.z : [132 x 1 x *7] }
	{ clonedmodel.HL2.W : [512 x 512] (gradient)
	  clonedmodel.HL3.t : [512 x 1 x *7]
	  clonedmodel.HL3.y : [512 x 1 x *7] }
	{ clonedmodel.HL2.b : [512 x 1] (gradient)
	  clonedmodel.HL2.y : [512 x 1 x *7] }

Here are the ones that don't share memory:
	{latticeAxis : [1 x 1 x latticeAxis]}
	{clonedmodel.OL.b : [132 x 1] (gradient)}
	{cr : [1] (gradient)}
	{cr : [1]}
	{features : [363 x *7]}
	{clonedmodel.globalInvStd : [363 x 1]}
	{clonedmodel.globalMean : [363 x 1]}
	{clonedmodel.globalPrior : [132 x 1]}
	{clonedmodel.HL1.b : [512 x 1]}
	{clonedmodel.HL1.W : [512 x 363]}
	{clonedmodel.HL2.b : [512 x 1]}
	{clonedmodel.HL2.W : [512 x 512]}
	{clonedmodel.HL3.b : [512 x 1]}
	{clonedmodel.HL3.W : [512 x 512]}
	{lattice : [1 x latticeAxis]}
	{clonedmodel.logPrior : [132 x 1]}
	{clonedmodel.HL3.b : [512 x 1] (gradient)}
	{labels : [132 x *7]}
	{clonedmodel.OL.W : [132 x 512]}
	{clonedmodel.OL.b : [132 x 1]}
	{Err : [1]}
	{clonedmodel.featNorm : [363 x *7]}


12/31/2017 19:37:21: Training 779396 parameters in 8 out of 8 parameter tensors and 21 nodes with gradient:

12/31/2017 19:37:21: 	Node 'clonedmodel.HL1.W' (LearnableParameter operation) : [512 x 363]
12/31/2017 19:37:21: 	Node 'clonedmodel.HL1.b' (LearnableParameter operation) : [512 x 1]
12/31/2017 19:37:21: 	Node 'clonedmodel.HL2.W' (LearnableParameter operation) : [512 x 512]
12/31/2017 19:37:21: 	Node 'clonedmodel.HL2.b' (LearnableParameter operation) : [512 x 1]
12/31/2017 19:37:21: 	Node 'clonedmodel.HL3.W' (LearnableParameter operation) : [512 x 512]
12/31/2017 19:37:21: 	Node 'clonedmodel.HL3.b' (LearnableParameter operation) : [512 x 1]
12/31/2017 19:37:21: 	Node 'clonedmodel.OL.W' (LearnableParameter operation) : [132 x 512]
12/31/2017 19:37:21: 	Node 'clonedmodel.OL.b' (LearnableParameter operation) : [132 x 1]

12/31/2017 19:37:21: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/31/2017 19:37:21: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.000000  momentum as time constant = 2432.7 samples

12/31/2017 19:37:21: Starting minibatch loop.
parallelforwardbackwardlattice: 65 launches for forward, 65 launches for backward
dengamma value 1.031630
dengamma value 0.996300
parallelforwardbackwardlattice: 94 launches for forward, 94 launches for backward
dengamma value 1.056201
parallelforwardbackwardlattice: 57 launches for forward, 57 launches for backward
dengamma value 1.059041
parallelforwardbackwardlattice: 76 launches for forward, 76 launches for backward
dengamma value 1.088895
parallelforwardbackwardlattice: 80 launches for forward, 80 launches for backward
dengamma value 1.029927
dengamma value 1.013481
dengamma value 0.960702
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.085838
parallelforwardbackwardlattice: 61 launches for forward, 61 launches for backward
dengamma value 1.105155
dengamma value 1.033577
dengamma value 0.961868
parallelforwardbackwardlattice: 33 launches for forward, 33 launches for backward
dengamma value 1.005383
dengamma value 1.000594
parallelforwardbackwardlattice: 51 launches for forward, 51 launches for backward
dengamma value 1.067961
parallelforwardbackwardlattice: 82 launches for forward, 82 launches for backward
dengamma value 1.042892
dengamma value 1.091392
parallelforwardbackwardlattice: 29 launches for forward, 29 launches for backward
dengamma value 0.986093
dengamma value 1.060506
dengamma value 1.072568
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.089808
dengamma value 1.093895
dengamma value 1.039661
parallelforwardbackwardlattice: 85 launches for forward, 85 launches for backward
dengamma value 1.058755
dengamma value 1.101132
dengamma value 1.068702
parallelforwardbackwardlattice: 91 launches for forward, 91 launches for backward
dengamma value 1.030079
dengamma value 1.000592
dengamma value 1.028220
dengamma value 0.986728
dengamma value 0.992012
12/31/2017 19:37:24: Finished Epoch[ 1 of 3]: [Training] cr = 0.08230535 * 9518; Err = 0.32653919 * 9518; totalSamplesSeen = 9518; learningRatePerSample = 2e-06; epochTime=2.89339s
12/31/2017 19:37:24: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.sequence.1'

12/31/2017 19:37:24: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.000000  momentum as time constant = 2432.7 samples

12/31/2017 19:37:24: Starting minibatch loop.
dengamma value 1.023233
dengamma value 1.060021
dengamma value 1.049919
WARNING: The same matrix with dim [1, 1224] has been transferred between different devices for 20 times.
parallelforwardbackwardlattice: 98 launches for forward, 98 launches for backward
dengamma value 1.078619
dengamma value 1.016137
dengamma value 1.007311
parallelforwardbackwardlattice: 32 launches for forward, 32 launches for backward
dengamma value 0.997840
parallelforwardbackwardlattice: 41 launches for forward, 41 launches for backward
dengamma value 1.082506
dengamma value 1.062728
dengamma value 1.178973
dengamma value 1.061650
parallelforwardbackwardlattice: 20 launches for forward, 20 launches for backward
dengamma value 1.063697
dengamma value 1.068759
dengamma value 1.040063
dengamma value 1.015481
dengamma value 1.035775
dengamma value 1.045306
parallelforwardbackwardlattice: 49 launches for forward, 49 launches for backward
dengamma value 1.043178
dengamma value 1.111398
dengamma value 1.058084
dengamma value 1.023321
parallelforwardbackwardlattice: 66 launches for forward, 66 launches for backward
dengamma value 1.182770
dengamma value 1.031702
dengamma value 0.982267
dengamma value 1.039824
parallelforwardbackwardlattice: 79 launches for forward, 79 launches for backward
dengamma value 1.053419
parallelforwardbackwardlattice: 48 launches for forward, 48 launches for backward
dengamma value 1.051022
parallelforwardbackwardlattice: 54 launches for forward, 54 launches for backward
dengamma value 0.963618
dengamma value 1.113633
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 1.003354
dengamma value 1.060290
dengamma value 1.015123
parallelforwardbackwardlattice: 53 launches for forward, 53 launches for backward
dengamma value 0.988930
dengamma value 1.050254
dengamma value 1.048348
dengamma value 1.022041
12/31/2017 19:37:26: Finished Epoch[ 2 of 3]: [Training] cr = 0.08259171 * 10188; Err = 0.29681979 * 10188; totalSamplesSeen = 19706; learningRatePerSample = 2e-06; epochTime=2.24388s
12/31/2017 19:37:26: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.sequence.2'

12/31/2017 19:37:26: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.000000  momentum as time constant = 2432.7 samples

12/31/2017 19:37:26: Starting minibatch loop.
parallelforwardbackwardlattice: 55 launches for forward, 55 launches for backward
dengamma value 0.998068
dengamma value 1.079818
dengamma value 1.035572
parallelforwardbackwardlattice: 93 launches for forward, 93 launches for backward
dengamma value 1.046579
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.092434
dengamma value 1.033453
dengamma value 1.034829
dengamma value 1.007747
dengamma value 1.066167
parallelforwardbackwardlattice: 58 launches for forward, 58 launches for backward
dengamma value 1.016879
dengamma value 1.024935
dengamma value 1.027340
parallelforwardbackwardlattice: 44 launches for forward, 44 launches for backward
dengamma value 1.096099
parallelforwardbackwardlattice: 123 launches for forward, 123 launches for backward
dengamma value 1.083479
parallelforwardbackwardlattice: 104 launches for forward, 104 launches for backward
dengamma value 1.092488
dengamma value 1.040069
dengamma value 0.982965
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 0.989375
parallelforwardbackwardlattice: 42 launches for forward, 42 launches for backward
dengamma value 1.026558
dengamma value 1.063844
dengamma value 1.081838
parallelforwardbackwardlattice: 59 launches for forward, 59 launches for backward
dengamma value 1.013275
parallelforwardbackwardlattice: 105 launches for forward, 105 launches for backward
dengamma value 1.034698
dengamma value 0.986427
dengamma value 1.075144
dengamma value 1.015325
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 0.940340
parallelforwardbackwardlattice: 28 launches for forward, 28 launches for backward
dengamma value 1.001910
dengamma value 0.999142
dengamma value 1.049789
dengamma value 1.140674
parallelforwardbackwardlattice: 64 launches for forward, 64 launches for backward
dengamma value 1.068060
parallelforwardbackwardlattice: 69 launches for forward, 69 launches for backward
dengamma value 0.980129
dengamma value 1.057569
dengamma value 1.045941
parallelforwardbackwardlattice: 37 launches for forward, 37 launches for backward
dengamma value 1.075895
12/31/2017 19:37:28: Finished Epoch[ 3 of 3]: [Training] cr = 0.08016216 * 9878; Err = 0.32405345 * 9878; totalSamplesSeen = 29584; learningRatePerSample = 2e-06; epochTime=2.23443s
12/31/2017 19:37:28: SGD: Saving checkpoint model '/tmp/cntk-test-20171231193452.438325/Speech/DNN_SequenceTrainingNewReader@debug_gpu/models/cntkSpeech.sequence'

12/31/2017 19:37:29: Action "train" complete.

12/31/2017 19:37:29: __COMPLETED__