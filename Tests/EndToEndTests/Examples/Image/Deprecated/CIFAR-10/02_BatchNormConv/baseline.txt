CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz
    Hardware threads: 12
    Total Memory: 57700428 kB
-------------------------------------------------------------------
=== Running /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv/02_BatchNormConv_ndl_deprecated.cntk currentDirectory=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/TestData RunDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu DataDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/TestData ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv OutputDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu DeviceId=-1 timestamping=true Train=[SGD=[maxEpochs=5]] Train=[SGD=[epochSize=100]] stderr=-
CNTK 2.3.1+ (HEAD f4f0f8, Dec 11 2017 18:34:12) at 2017/12/12 05:52:49

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv/02_BatchNormConv_ndl_deprecated.cntk  currentDirectory=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/TestData  RunDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu  DataDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv  OutputDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu  DeviceId=-1  timestamping=true  Train=[SGD=[maxEpochs=5]]  Train=[SGD=[epochSize=100]]  stderr=-
Changed current directory to /tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/TestData
12/12/2017 05:52:49: Redirecting stderr to file -_Train_Test.log
12/12/2017 05:52:49: -------------------------------------------------------------------
12/12/2017 05:52:49: Build info: 

12/12/2017 05:52:49: 		Built time: Dec 11 2017 18:28:39
12/12/2017 05:52:49: 		Last modified date: Wed Nov 15 09:27:10 2017
12/12/2017 05:52:49: 		Build type: release
12/12/2017 05:52:49: 		Build target: GPU
12/12/2017 05:52:49: 		With 1bit-SGD: yes
12/12/2017 05:52:49: 		With ASGD: yes
12/12/2017 05:52:49: 		Math lib: mkl
12/12/2017 05:52:49: 		CUDA version: 9.0.0
12/12/2017 05:52:49: 		CUDNN version: 7.0.4
12/12/2017 05:52:49: 		Build Branch: HEAD
12/12/2017 05:52:49: 		Build SHA1: f4f0f82eabcc482dbd03af1f946a44ae2b8b97bf
12/12/2017 05:52:49: 		MPI distribution: Open MPI
12/12/2017 05:52:49: 		MPI version: 1.10.7
12/12/2017 05:52:49: -------------------------------------------------------------------
12/12/2017 05:52:49: -------------------------------------------------------------------
12/12/2017 05:52:49: GPU info:

12/12/2017 05:52:49: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 8112 MB
12/12/2017 05:52:49: -------------------------------------------------------------------

Configuration After Processing and Variable Resolution:

configparameters: 02_BatchNormConv_ndl_deprecated.cntk:command=Train:Test
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:currentDirectory=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/TestData
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:DataDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/TestData
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:deviceId=-1
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:imageLayout=cudnn
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:initOnCPUOnly=true
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:ModelDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/Models
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:ndlMacros=/home/ubuntu/workspace/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv/../Macros.ndl
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:numMBsToShowResult=500
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:OutputDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:precision=float
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:RootDir=.
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:RunDir=/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:stderr=-
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:Test=[
    action = "test"
    modelPath = "/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/Models/02_BatchNormConv"
    minibatchSize = 16
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/TestData/Test_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
]

configparameters: 02_BatchNormConv_ndl_deprecated.cntk:timestamping=true
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:traceLevel=1
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:Train=[
    action = "train"
    modelPath = "/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/Models/02_BatchNormConv"
     NDLNetworkBuilder = [
        networkDescription = "/home/ubuntu/workspace/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv/02_BatchNormConv.ndl"
    ]
    SGD = [
        epochSize = 49984
        minibatchSize = 64
        learningRatesPerMB = 0.03*7:0.01
        momentumPerMB = 0
        maxEpochs = 10
        L2RegWeight = 0
        dropoutRate = 0
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/TestData/Train_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
] [SGD=[maxEpochs=5]] [SGD=[epochSize=100]]

12/12/2017 05:52:49: Commands: Train Test
12/12/2017 05:52:49: precision = "float"

12/12/2017 05:52:49: ##############################################################################
12/12/2017 05:52:49: #                                                                            #
12/12/2017 05:52:49: # Train command (train action)                                               #
12/12/2017 05:52:49: #                                                                            #
12/12/2017 05:52:49: ##############################################################################

12/12/2017 05:52:49: 
Creating virgin network.
NDLBuilder Using CPU
conv1.c.c.c: using GEMM convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 32, Kernel: 5 x 5 x 3, Map: 1 x 1 x 32, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool1: using GEMM convolution engine for geometry: Input: 32 x 32 x 32, Output: 15 x 15 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv2.c.c.c: using GEMM convolution engine for geometry: Input: 15 x 15 x 32, Output: 15 x 15 x 32, Kernel: 5 x 5 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool2: using GEMM convolution engine for geometry: Input: 15 x 15 x 32, Output: 7 x 7 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv3.c.c.c: using GEMM convolution engine for geometry: Input: 7 x 7 x 32, Output: 7 x 7 x 64, Kernel: 5 x 5 x 32, Map: 1 x 1 x 64, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool3: using GEMM convolution engine for geometry: Input: 7 x 7 x 64, Output: 3 x 3 x 64, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
Using CNTK batch normalization engine.
12/12/2017 05:52:49: 
Model has 49 nodes. Using CPU.

12/12/2017 05:52:49: Training criterion:   CE = CrossEntropyWithSoftmax
12/12/2017 05:52:49: Evaluation criterion: Err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 4 are aliased.
	features (gradient) reuses featScaled (gradient)
	OutputNodes.t (gradient) reuses OutputNodes.z (gradient)

Memory Sharing: Out of 81 matrices, 43 are shared as 12, and 38 are not shared.

Here are the ones that share memory:
	{ conv1.c.c.c : [32 x 32 x 32 x *] (gradient)
	  conv1.y : [32 x 32 x 32 x *] }
	{ conv1.c.c.sc : [32 x 1] (gradient)
	  conv2.c.c.y : [15 x 15 x 32 x *]
	  conv2.c.c.y : [15 x 15 x 32 x *] (gradient)
	  conv3.c.c.c : [7 x 7 x 64 x *] (gradient)
	  conv3.y : [7 x 7 x 64 x *] (gradient)
	  h1.t : [64 x *]
	  pool1 : [15 x 15 x 32 x *] (gradient) }
	{ conv2.c.W : [32 x 800] (gradient)
	  conv2.y : [15 x 15 x 32 x *] (gradient)
	  conv3.c.c.c : [7 x 7 x 64 x *] }
	{ conv1.c.W : [32 x 75] (gradient)
	  conv1.c.c.y : [32 x 32 x 32 x *]
	  conv1.y : [32 x 32 x 32 x *] (gradient)
	  conv2.c.c.c : [15 x 15 x 32 x *] }
	{ conv1.c.c.y : [32 x 32 x 32 x *] (gradient)
	  pool1 : [15 x 15 x 32 x *] }
	{ conv2.c.c.sc : [32 x 1] (gradient)
	  conv3.y : [7 x 7 x 64 x *]
	  pool2 : [7 x 7 x 32 x *] (gradient) }
	{ conv3.c.c.b : [64 x 1] (gradient)
	  h1.y : [64 x *]
	  pool3 : [3 x 3 x 64 x *] (gradient) }
	{ OutputNodes.z : [10 x *]
	  conv3.c.c.sc : [64 x 1] (gradient)
	  h1.bn : [64 x *]
	  h1.t : [64 x *] (gradient)
	  h1.y : [64 x *] (gradient) }
	{ conv2.c.c.b : [32 x 1] (gradient)
	  pool2 : [7 x 7 x 32 x *] }
	{ conv3.c.W : [64 x 800] (gradient)
	  conv3.c.c.y : [7 x 7 x 64 x *]
	  conv3.c.c.y : [7 x 7 x 64 x *] (gradient)
	  pool3 : [3 x 3 x 64 x *] }
	{ OutputNodes.t : [10 x *]
	  OutputNodes.t : [10 x *] (gradient)
	  OutputNodes.z : [10 x *] (gradient)
	  h1.W : [64 x 3 x 3 x 64] (gradient)
	  h1.bn : [64 x *] (gradient) }
	{ conv1.c.c.b : [32 x 1] (gradient)
	  conv2.c.c.c : [15 x 15 x 32 x *] (gradient)
	  conv2.y : [15 x 15 x 32 x *] }

Here are the ones that don't share memory:
	{labels : [10 x *]}
	{features : [32 x 32 x 3 x *]}
	{featOffs : [1 x 1]}
	{conv1.c.W : [32 x 75]}
	{conv1.c.c.b : [32 x 1]}
	{conv1.c.c.sc : [32 x 1]}
	{conv1.c.c.m : [32 x 1]}
	{conv1.c.c.v : [32 x 1]}
	{conv1.c.c.y.run_sample_count : [1]}
	{conv2.c.W : [32 x 800]}
	{conv2.c.c.b : [32 x 1]}
	{conv2.c.c.sc : [32 x 1]}
	{conv2.c.c.m : [32 x 1]}
	{conv2.c.c.v : [32 x 1]}
	{conv2.c.c.y.run_sample_count : [1]}
	{conv3.c.W : [64 x 800]}
	{conv3.c.c.b : [64 x 1]}
	{conv3.c.c.sc : [64 x 1]}
	{conv3.c.c.m : [64 x 1]}
	{conv3.c.c.v : [64 x 1]}
	{conv3.c.c.y.run_sample_count : [1]}
	{h1.W : [64 x 3 x 3 x 64]}
	{h1.b : [64 x 1]}
	{h1.sc : [64 x 1]}
	{h1.m : [64 x 1]}
	{h1.v : [64 x 1]}
	{h1.bn.run_sample_count : [1]}
	{OutputNodes.W : [10 x 64]}
	{OutputNodes.b : [10]}
	{CE : [1]}
	{conv1.c.c.c : [32 x 32 x 32 x *]}
	{OutputNodes.W : [10 x 64] (gradient)}
	{h1.b : [64 x 1] (gradient)}
	{OutputNodes.b : [10] (gradient)}
	{featScaled : [32 x 32 x 3 x *]}
	{CE : [1] (gradient)}
	{h1.sc : [64 x 1] (gradient)}
	{Err : [1]}


12/12/2017 05:52:49: Training 117098 parameters in 14 out of 14 parameter tensors and 32 nodes with gradient:

12/12/2017 05:52:49: 	Node 'OutputNodes.W' (LearnableParameter operation) : [10 x 64]
12/12/2017 05:52:49: 	Node 'OutputNodes.b' (LearnableParameter operation) : [10]
12/12/2017 05:52:49: 	Node 'conv1.c.W' (LearnableParameter operation) : [32 x 75]
12/12/2017 05:52:49: 	Node 'conv1.c.c.b' (LearnableParameter operation) : [32 x 1]
12/12/2017 05:52:49: 	Node 'conv1.c.c.sc' (LearnableParameter operation) : [32 x 1]
12/12/2017 05:52:49: 	Node 'conv2.c.W' (LearnableParameter operation) : [32 x 800]
12/12/2017 05:52:49: 	Node 'conv2.c.c.b' (LearnableParameter operation) : [32 x 1]
12/12/2017 05:52:49: 	Node 'conv2.c.c.sc' (LearnableParameter operation) : [32 x 1]
12/12/2017 05:52:49: 	Node 'conv3.c.W' (LearnableParameter operation) : [64 x 800]
12/12/2017 05:52:49: 	Node 'conv3.c.c.b' (LearnableParameter operation) : [64 x 1]
12/12/2017 05:52:49: 	Node 'conv3.c.c.sc' (LearnableParameter operation) : [64 x 1]
12/12/2017 05:52:49: 	Node 'h1.W' (LearnableParameter operation) : [64 x 3 x 3 x 64]
12/12/2017 05:52:49: 	Node 'h1.b' (LearnableParameter operation) : [64 x 1]
12/12/2017 05:52:49: 	Node 'h1.sc' (LearnableParameter operation) : [64 x 1]

12/12/2017 05:52:49: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/12/2017 05:52:49: Starting Epoch 1: learning rate per sample = 0.000469  effective momentum = 0.000000  momentum as time constant = 0.0 samples

12/12/2017 05:52:49: Starting minibatch loop.
12/12/2017 05:52:53: Finished Epoch[ 1 of 5]: [Training] CE = 2.34727600 * 100; Err = 0.87000000 * 100; totalSamplesSeen = 100; learningRatePerSample = 0.00046874999; epochTime=4.3368s
12/12/2017 05:52:53: SGD: Saving checkpoint model '/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/Models/02_BatchNormConv.1'

12/12/2017 05:52:53: Starting Epoch 2: learning rate per sample = 0.000469  effective momentum = 0.000000  momentum as time constant = 0.0 samples

12/12/2017 05:52:53: Starting minibatch loop.
12/12/2017 05:52:54: Finished Epoch[ 2 of 5]: [Training] CE = 2.24859177 * 100; Err = 0.81000000 * 100; totalSamplesSeen = 200; learningRatePerSample = 0.00046874999; epochTime=0.318268s
12/12/2017 05:52:54: SGD: Saving checkpoint model '/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/Models/02_BatchNormConv.2'

12/12/2017 05:52:54: Starting Epoch 3: learning rate per sample = 0.000469  effective momentum = 0.000000  momentum as time constant = 0.0 samples

12/12/2017 05:52:54: Starting minibatch loop.
12/12/2017 05:52:54: Finished Epoch[ 3 of 5]: [Training] CE = 2.22076538 * 100; Err = 0.80000000 * 100; totalSamplesSeen = 300; learningRatePerSample = 0.00046874999; epochTime=0.122527s
12/12/2017 05:52:54: SGD: Saving checkpoint model '/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/Models/02_BatchNormConv.3'

12/12/2017 05:52:54: Starting Epoch 4: learning rate per sample = 0.000469  effective momentum = 0.000000  momentum as time constant = 0.0 samples

12/12/2017 05:52:54: Starting minibatch loop.
12/12/2017 05:52:54: Finished Epoch[ 4 of 5]: [Training] CE = 2.20701019 * 100; Err = 0.76000000 * 100; totalSamplesSeen = 400; learningRatePerSample = 0.00046874999; epochTime=0.120628s
12/12/2017 05:52:54: SGD: Saving checkpoint model '/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/Models/02_BatchNormConv.4'

12/12/2017 05:52:54: Starting Epoch 5: learning rate per sample = 0.000469  effective momentum = 0.000000  momentum as time constant = 0.0 samples

12/12/2017 05:52:54: Starting minibatch loop.
12/12/2017 05:52:54: Finished Epoch[ 5 of 5]: [Training] CE = 2.16617828 * 100; Err = 0.78000000 * 100; totalSamplesSeen = 500; learningRatePerSample = 0.00046874999; epochTime=0.122139s
12/12/2017 05:52:54: SGD: Saving checkpoint model '/tmp/cntk-test-20171211223423.932710/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_cpu/Models/02_BatchNormConv'

12/12/2017 05:52:54: Action "train" complete.


12/12/2017 05:52:54: ##############################################################################
12/12/2017 05:52:54: #                                                                            #
12/12/2017 05:52:54: # Test command (test action)                                                 #
12/12/2017 05:52:54: #                                                                            #
12/12/2017 05:52:54: ##############################################################################


Post-processing network...

3 roots:
	CE = CrossEntropyWithSoftmax()
	Err = ClassificationError()
	OutputNodes.z = Plus()

Validating network. 49 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [10 x *1]
Validating --> OutputNodes.W = LearnableParameter() :  -> [10 x 64]
Validating --> h1.W = LearnableParameter() :  -> [64 x 3 x 3 x 64]
Validating --> conv3.c.W = LearnableParameter() :  -> [64 x 800]
Validating --> conv2.c.W = LearnableParameter() :  -> [32 x 800]
Validating --> conv1.c.W = LearnableParameter() :  -> [32 x 75]
Validating --> features = InputValue() :  -> [32 x 32 x 3 x *1]
Validating --> featOffs = LearnableParameter() :  -> [1 x 1]
Validating --> featScaled = Minus (features, featOffs) : [32 x 32 x 3 x *1], [1 x 1] -> [32 x 32 x 3 x *1]
Validating --> conv1.c.c.c = Convolution (conv1.c.W, featScaled) : [32 x 75], [32 x 32 x 3 x *1] -> [32 x 32 x 32 x *1]
Validating --> conv1.c.c.sc = LearnableParameter() :  -> [32 x 1]
Validating --> conv1.c.c.b = LearnableParameter() :  -> [32 x 1]
Validating --> conv1.c.c.m = LearnableParameter() :  -> [32 x 1]
Validating --> conv1.c.c.v = LearnableParameter() :  -> [32 x 1]
Validating --> conv1.c.c.y.run_sample_count = LearnableParameter() :  -> [1]
Validating --> conv1.c.c.y = BatchNormalization (conv1.c.c.c, conv1.c.c.sc, conv1.c.c.b, conv1.c.c.m, conv1.c.c.v, conv1.c.c.y.run_sample_count) : [32 x 32 x 32 x *1], [32 x 1], [32 x 1], [32 x 1], [32 x 1], [1] -> [32 x 32 x 32 x *1]
Validating --> conv1.y = RectifiedLinear (conv1.c.c.y) : [32 x 32 x 32 x *1] -> [32 x 32 x 32 x *1]
Validating --> pool1 = MaxPooling (conv1.y) : [32 x 32 x 32 x *1] -> [15 x 15 x 32 x *1]
Validating --> conv2.c.c.c = Convolution (conv2.c.W, pool1) : [32 x 800], [15 x 15 x 32 x *1] -> [15 x 15 x 32 x *1]
Validating --> conv2.c.c.sc = LearnableParameter() :  -> [32 x 1]
Validating --> conv2.c.c.b = LearnableParameter() :  -> [32 x 1]
Validating --> conv2.c.c.m = LearnableParameter() :  -> [32 x 1]
Validating --> conv2.c.c.v = LearnableParameter() :  -> [32 x 1]
Validating --> conv2.c.c.y.run_sample_count = LearnableParameter() :  -> [1]
Validating --> conv2.c.c.y = BatchNormalization (conv2.c.c.c, conv2.c.c.sc, conv2.c.c.b, conv2.c.c.m, conv2.c.c.v, conv2.c.c.y.run_sample_count) : [15 x 15 x 32 x *1], [32 x 1], [32 x 1], [32 x 1], [32 x 1], [1] -> [15 x 15 x 32 x *1]
Validating --> conv2.y = RectifiedLinear (conv2.c.c.y) : [15 x 15 x 32 x *1] -> [15 x 15 x 32 x *1]
Validating --> pool2 = MaxPooling (conv2.y) : [15 x 15 x 32 x *1] -> [7 x 7 x 32 x *1]
Validating --> conv3.c.c.c = Convolution (conv3.c.W, pool2) : [64 x 800], [7 x 7 x 32 x *1] -> [7 x 7 x 64 x *1]
Validating --> conv3.c.c.sc = LearnableParameter() :  -> [64 x 1]
Validating --> conv3.c.c.b = LearnableParameter() :  -> [64 x 1]
Validating --> conv3.c.c.m = LearnableParameter() :  -> [64 x 1]
Validating --> conv3.c.c.v = LearnableParameter() :  -> [64 x 1]
Validating --> conv3.c.c.y.run_sample_count = LearnableParameter() :  -> [1]
Validating --> conv3.c.c.y = BatchNormalization (conv3.c.c.c, conv3.c.c.sc, conv3.c.c.b, conv3.c.c.m, conv3.c.c.v, conv3.c.c.y.run_sample_count) : [7 x 7 x 64 x *1], [64 x 1], [64 x 1], [64 x 1], [64 x 1], [1] -> [7 x 7 x 64 x *1]
Validating --> conv3.y = RectifiedLinear (conv3.c.c.y) : [7 x 7 x 64 x *1] -> [7 x 7 x 64 x *1]
Validating --> pool3 = MaxPooling (conv3.y) : [7 x 7 x 64 x *1] -> [3 x 3 x 64 x *1]
Validating --> h1.t = Times (h1.W, pool3) : [64 x 3 x 3 x 64], [3 x 3 x 64 x *1] -> [64 x *1]
Validating --> h1.sc = LearnableParameter() :  -> [64 x 1]
Validating --> h1.b = LearnableParameter() :  -> [64 x 1]
Validating --> h1.m = LearnableParameter() :  -> [64 x 1]
Validating --> h1.v = LearnableParameter() :  -> [64 x 1]
Validating --> h1.bn.run_sample_count = LearnableParameter() :  -> [1]
Validating --> h1.bn = BatchNormalization (h1.t, h1.sc, h1.b, h1.m, h1.v, h1.bn.run_sample_count) : [64 x *1], [64 x 1], [64 x 1], [64 x 1], [64 x 1], [1] -> [64 x *1]
Validating --> h1.y = RectifiedLinear (h1.bn) : [64 x *1] -> [64 x *1]
Validating --> OutputNodes.t = Times (OutputNodes.W, h1.y) : [10 x 64], [64 x *1] -> [10 x *1]
Validating --> OutputNodes.b = LearnableParameter() :  -> [10]
Validating --> OutputNodes.z = Plus (OutputNodes.t, OutputNodes.b) : [10 x *1], [10] -> [10 x *1]
Validating --> CE = CrossEntropyWithSoftmax (labels, OutputNodes.z) : [10 x *1], [10 x *1] -> [1]
Validating --> Err = ClassificationError (labels, OutputNodes.z) : [10 x *1], [10 x *1] -> [1]

Validating network. 20 nodes to process in pass 2.


Validating network, final pass.

conv1.c.c.c: using GEMM convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 32, Kernel: 5 x 5 x 3, Map: 1 x 1 x 32, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool1: using GEMM convolution engine for geometry: Input: 32 x 32 x 32, Output: 15 x 15 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv2.c.c.c: using GEMM convolution engine for geometry: Input: 15 x 15 x 32, Output: 15 x 15 x 32, Kernel: 5 x 5 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool2: using GEMM convolution engine for geometry: Input: 15 x 15 x 32, Output: 7 x 7 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv3.c.c.c: using GEMM convolution engine for geometry: Input: 7 x 7 x 32, Output: 7 x 7 x 64, Kernel: 5 x 5 x 32, Map: 1 x 1 x 64, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool3: using GEMM convolution engine for geometry: Input: 7 x 7 x 64, Output: 3 x 3 x 64, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
Using CNTK batch normalization engine.



Post-processing network complete.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 49 matrices, 18 are shared as 3, and 31 are not shared.

Here are the ones that share memory:
	{ OutputNodes.t : [10 x *1]
	  conv1.c.c.y : [32 x 32 x 32 x *1]
	  conv2.y : [15 x 15 x 32 x *1]
	  conv3.c.c.c : [7 x 7 x 64 x *1]
	  conv3.y : [7 x 7 x 64 x *1]
	  h1.t : [64 x *1]
	  pool1 : [15 x 15 x 32 x *1] }
	{ OutputNodes.z : [10 x *1]
	  conv2.c.c.y : [15 x 15 x 32 x *1]
	  conv3.c.c.y : [7 x 7 x 64 x *1]
	  featScaled : [32 x 32 x 3 x *1]
	  h1.y : [64 x *1]
	  pool2 : [7 x 7 x 32 x *1]
	  pool3 : [3 x 3 x 64 x *1] }
	{ conv1.c.c.c : [32 x 32 x 32 x *1]
	  conv1.y : [32 x 32 x 32 x *1]
	  conv2.c.c.c : [15 x 15 x 32 x *1]
	  h1.bn : [64 x *1] }

Here are the ones that don't share memory:
	{h1.b : [64 x 1]}
	{labels : [10 x *1]}
	{OutputNodes.b : [10]}
	{conv1.c.c.v : [32 x 1]}
	{conv3.c.c.m : [64 x 1]}
	{h1.bn.run_sample_count : [1]}
	{h1.m : [64 x 1]}
	{conv1.c.c.m : [32 x 1]}
	{conv3.c.c.sc : [64 x 1]}
	{conv1.c.c.b : [32 x 1]}
	{conv3.c.c.b : [64 x 1]}
	{conv2.c.c.y.run_sample_count : [1]}
	{Err : [1]}
	{h1.sc : [64 x 1]}
	{conv1.c.W : [32 x 75]}
	{conv1.c.c.y.run_sample_count : [1]}
	{conv2.c.c.v : [32 x 1]}
	{h1.v : [64 x 1]}
	{h1.W : [64 x 3 x 3 x 64]}
	{conv3.c.c.v : [64 x 1]}
	{CE : [1]}
	{conv3.c.c.y.run_sample_count : [1]}
	{conv3.c.W : [64 x 800]}
	{conv2.c.c.m : [32 x 1]}
	{conv2.c.W : [32 x 800]}
	{conv2.c.c.b : [32 x 1]}
	{featOffs : [1 x 1]}
	{features : [32 x 32 x 3 x *1]}
	{conv2.c.c.sc : [32 x 1]}
	{conv1.c.c.sc : [32 x 1]}
	{OutputNodes.W : [10 x 64]}

12/12/2017 05:52:58: Minibatch[1-500]: Err = 0.83650000 * 8000; CE = 4.09384346 * 8000
12/12/2017 05:52:58: Minibatch[501-625]: Err = 0.83300000 * 2000; CE = 4.15290434 * 2000
12/12/2017 05:52:58: Final Results: Minibatch[1-625]: Err = 0.83580000 * 10000; CE = 4.10565563 * 10000; perplexity = 60.68251691

12/12/2017 05:52:58: Action "test" complete.

12/12/2017 05:52:58: __COMPLETED__